prefix: n4_3_github


activation: F.relu
device: cuda:0
hidden_layers:
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512
- 512

lr_start: 0.0004
optimizer: torch.optim.Adam

# optimizer params for Adamw
# optimizer_params:
#   weight_decay: 0.0001



patience_model: 75
training_size: 1000000
validation_size: 1000000
testing_size: 5000000

# scheduler: exponential
# scheduler_params:
#   gamma: 0.99
#   last_epoch: -1
#   verbose: false

scheduler: plateau
scheduler_params:
  lr_patience: 10
  factor: 0.5
  
train_batch_size: 320
