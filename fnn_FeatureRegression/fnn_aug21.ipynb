{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Sampler\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../FakeDatasetMaking/')\n",
    "from pair_nomet_creation_temp import KinematicDataset, EpochSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsavepath='/home/ddemler/HNLclassifier/saved_files/saved_models/FNN_FeatureRegression/fnn_aug21all.pt'\n",
    "pdsavepath='/home/ddemler/HNLclassifier/saved_files/fnn_featregr/fnn_aug21all2.csv'\n",
    "\n",
    "out_feats=['deltaphi', 'deltaeta', 'deltaR', 'mt', 'norm_mt', 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n",
    "tryrel=[ 'mt','mass' 'pt', 'px', 'py', 'pz', 'energy']\n",
    "customlossindices=[idx for idx, feat in enumerate(out_feats) if feat in tryrel]\n",
    "\n",
    "# hidden_layers = [64 for i in range(20)]\n",
    "hidden_layers = [32 for i in range(5)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KinematicDataset(num_events=1000000, seed=0)\n",
    "input_dim, output_dim = train_dataset.usefulvariables()\n",
    "# print(input_dim, output_dim)\n",
    "\n",
    "# print(input_dim, output_dim)\n",
    "\n",
    "train_sampler = EpochSampler(train_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=320, sampler=train_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = KinematicDataset(num_events=500000, seed=10000)\n",
    "input_dim, output_dim = val_dataset.usefulvariables()\n",
    "\n",
    "val_sampler = EpochSampler(val_dataset)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=320, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CustomKinematicNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, lenoutput, activation_fn=F.relu):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - input_size (int): Size of the input layer.\n",
    "        - hidden_layers (list of int): Sizes of each hidden layer.\n",
    "        - lenoutput (int): Size of the output layer.\n",
    "        - activation_fn (callable): Activation function to use.\n",
    "        \"\"\"\n",
    "        super(CustomKinematicNet, self).__init__()\n",
    "        \n",
    "        # Create the list of layers\n",
    "        layers = [nn.Linear(input_size, hidden_layers[0])]\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i + 1]))\n",
    "        layers.append(nn.Linear(hidden_layers[-1], lenoutput))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.activation_fn = activation_fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation_fn(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "    \n",
    "\n",
    "\n",
    "def custom_loss(y_pred, y_true):\n",
    "    se_loss = (y_pred - y_true) ** 2\n",
    "    MSE_loss = torch.zeros_like(se_loss)  # Initialize with zeros\n",
    "    \n",
    "    num_features = int(output_dim)\n",
    "    loss_list = []\n",
    "\n",
    "    for i in range(num_features):\n",
    "        if i in customlossindices:\n",
    "            RMSE = ((y_pred[:, i] - y_true[:, i]) / y_true[:, i]) ** 2\n",
    "            mask = (y_true[:, i] > 1)\n",
    "            \n",
    "            RMSE_meanloss = torch.mean(RMSE[mask])\n",
    "            MSE_meanloss = torch.mean(se_loss[:, i][~mask])\n",
    "\n",
    "            # Weighted contribution of the RMSE for the masked values to the final loss tensor\n",
    "            MSE_loss[:, i] = RMSE * mask.float()\n",
    "\n",
    "            loss_list.append(MSE_meanloss.item())\n",
    "            loss_list.append(RMSE_meanloss.item())\n",
    "        else:\n",
    "            loss = torch.mean(se_loss[:, i])\n",
    "            loss_list.append(loss.item())\n",
    "            MSE_loss[:, i] = se_loss[:, i]  # Copy over the entire squared error for this feature\n",
    "\n",
    "    full_loss = torch.mean(MSE_loss)  # Calculate the final average loss\n",
    "    return loss_list, full_loss\n",
    "\n",
    "\n",
    "# def custom_loss(y_pred, y_true):\n",
    "#     se_loss = (y_pred - y_true) ** 2\n",
    "#     # print(se_loss.shape)\n",
    "#     num_features = int(output_dim)\n",
    "\n",
    "#     loss_list=[]\n",
    "\n",
    "#     for i in range(num_features):\n",
    "\n",
    "#         if i in customlossindices:\n",
    "#             RMSE=((y_pred[:,i]-y_true[:,i])/y_true[:,i])**2\n",
    "#             mask = (y_true[:,i] > 1)\n",
    "\n",
    "#             RMSE_meanloss=torch.mean(RMSE[mask])\n",
    "#             MSE_meanloss = torch.mean(se_loss[:,i][~mask])\n",
    "\n",
    "#             se_loss[mask, i] = RMSE[mask]\n",
    "\n",
    "#             loss_list.append(MSE_meanloss.item())\n",
    "#             loss_list.append(RMSE_meanloss.item())\n",
    "#         else:\n",
    "#             loss = torch.mean(se_loss[:,i])\n",
    "#             loss_list.append(loss.item())\n",
    "    \n",
    "#     full_loss = torch.mean(se_loss)\n",
    "#     return loss_list, full_loss\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# def l2_regularization(model, lambda_reg):\n",
    "#     l2_reg = 0.0\n",
    "#     for W in model.parameters():\n",
    "#         l2_reg += torch.sum(W ** 2)\n",
    "#     return l2_reg * lambda_reg      \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomKinematicNet(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=8, out_features=32, bias=True)\n",
       "    (1-4): 4 x Linear(in_features=32, out_features=32, bias=True)\n",
       "    (5): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden_layers=[64,72,82,92,102,112,122,132,142,132,122,112,102,92,82,72,64]\n",
    "# hidden_layers=[64,72,82,92,102,112,122,132,142,132,102,92,82]\n",
    "\n",
    "model = CustomKinematicNet(input_size=input_dim, hidden_layers=hidden_layers, lenoutput=output_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss', 'val_loss', 'l2sum', 'deltaphi_MSE', 'deltaeta_MSE', 'deltaR_MSE', 'mt_MSE', 'mt_RMSE', 'norm_mt_MSE', 'mass_MSE', 'pt_MSE', 'eta_MSE', 'phi_MSE', 'px_MSE', 'px_RMSE', 'py_MSE', 'py_RMSE', 'pz_MSE', 'pz_RMSE', 'energy_MSE', 'energy_RMSE']\n"
     ]
    }
   ],
   "source": [
    "out_feats=['deltaphi', 'deltaeta', 'deltaR', 'mt', 'norm_mt', 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n",
    "\n",
    "df_outfeats=[]\n",
    "for i, feat in enumerate(out_feats):\n",
    "    if i in customlossindices:\n",
    "        df_outfeats.append(feat +\"_MSE\")\n",
    "        df_outfeats.append(feat +\"_RMSE\")\n",
    "    else:\n",
    "        df_outfeats.append(feat +\"_MSE\")\n",
    "\n",
    "losses_cols=['train_loss', 'val_loss', 'l2sum']+df_outfeats\n",
    "\n",
    "print(losses_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l2_regularization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m train_featslosssep, original_loss \u001b[39m=\u001b[39m custom_loss(y_pred, y)\n\u001b[1;32m     31\u001b[0m \u001b[39m# train_featsloss_list.append(train_featslosssep.cpu().numpy())\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m l2_loss \u001b[39m=\u001b[39m l2_regularization(model, lambda_reg\u001b[39m=\u001b[39m\u001b[39m1e-7\u001b[39m)\n\u001b[1;32m     33\u001b[0m loss \u001b[39m=\u001b[39m original_loss \u001b[39m+\u001b[39m l2_loss\n\u001b[1;32m     34\u001b[0m l2sum\u001b[39m+\u001b[39m\u001b[39m=\u001b[39ml2_loss\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l2_regularization' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# loss_fn=nn.MSELoss()\n",
    "\n",
    "\n",
    "out_feats=['deltaphi', 'deltaeta', 'deltaR', 'mt', 'norm_mt', 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n",
    "\n",
    "df_outfeats=[]\n",
    "for i, feat in enumerate(out_feats):\n",
    "    if i in customlossindices:\n",
    "        df_outfeats.append(feat +\"_MSE\")\n",
    "        df_outfeats.append(feat +\"_RMSE\")\n",
    "    else:\n",
    "        df_outfeats.append(feat +\"_MSE\")\n",
    "\n",
    "losses_cols=['train_loss', 'val_loss', 'l2sum']+df_outfeats\n",
    "losses_df=pd.DataFrame(columns=losses_cols)\n",
    "\n",
    "\n",
    "numepochs=10000\n",
    "best_loss=np.inf\n",
    "for epoch in range(numepochs):\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    l2sum=0\n",
    "    # train_featsloss_list=[]\n",
    "    for i, (x, y) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False, position=0, disable=True):\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        y_pred=model(x)\n",
    "        train_featslosssep, original_loss = custom_loss(y_pred, y)\n",
    "        # train_featsloss_list.append(train_featslosssep.cpu().numpy())\n",
    "        l2_loss = l2_regularization(model, lambda_reg=1e-7)\n",
    "        loss = original_loss + l2_loss\n",
    "        l2sum+=l2_loss.item()\n",
    "        train_loss += original_loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # train_featsloss_array = np.array(train_featsloss_list)\n",
    "    # avg_train_featsloss = np.mean(train_featsloss_array, axis=0)\n",
    "    \n",
    "    model.eval()\n",
    "    # patience=20\n",
    "    with torch.no_grad():\n",
    "        x,y=next(iter(val_loader))\n",
    "        # x=val_loader\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        y_pred=model(x)\n",
    "        feats_loss, valloss = custom_loss(y_pred, y)\n",
    "        # valloss=sum(feats_loss)/len(feats_loss)\n",
    "        \n",
    "\n",
    "        if valloss<best_loss:\n",
    "            best_loss=valloss\n",
    "            patience=30\n",
    "            modelsave=deepcopy(model.state_dict())\n",
    "            torch.save(modelsave, modelsavepath)\n",
    "        else:\n",
    "            patience-=1\n",
    "            if patience==0:\n",
    "                print('early stopping')\n",
    "                break\n",
    "    # indice=[3,6,12]\n",
    "    # for idx in indice:\n",
    "    #     feats_loss[idx]=feats_loss[idx].cpu().float()\n",
    "    \n",
    "    loss_strings = [f\"{df_outfeats[i]}: {feats_loss[i]:.4e}\" for i in range(len(df_outfeats))]\n",
    "    loss_summary = \", \".join(loss_strings)\n",
    "    loss_values = [train_loss/len(train_loader), valloss.item(), l2sum]\n",
    "    loss_values.extend(feats_loss)\n",
    "\n",
    "    losses_df.loc[epoch] = loss_values\n",
    "    losses_df.to_csv(pdsavepath)\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"epoch: {epoch}, train: {train_loss/len(train_loader):.4e}, val: {valloss.item():.4e}, {loss_summary}\")\n",
    "\n",
    "    # valloss=valloss.cpu().float()\n",
    "    # valloss2=valloss.item()\n",
    "    # loss_strings = [f\"{out_feats[i]}: {feats_loss[i]:.4e}\" for i in range(len(out_feats))]\n",
    "    # loss_summary = \", \".join(loss_strings)\n",
    "    # loss_values = [train_loss/len(train_loader), valloss2, l2sum]\n",
    "    # loss_values.extend(feats_loss)\n",
    "    # # for losses in loss_values:\n",
    "    #     # print(type(losses))\n",
    "    # losses_df.loc[epoch] = loss_values\n",
    "    # losses_df.to_csv(pdsavepath)\n",
    "    # # losses_df.loc[epoch]=[train_loss/len(train_loader), valloss2, feats_loss[0], feats_loss[1]]\n",
    "    # print(f\"epoch: {epoch}, train: {train_loss/len(train_loader):.4e}, val: {valloss2:.4e}, {loss_summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for seed: 10000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3000000 into shape (1500000,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m y_curr\u001b[39m=\u001b[39my_total[:,:,i]\n\u001b[1;32m     42\u001b[0m \u001b[39m# print(\"ycurr shape before reshape\", y_curr.shape)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m y_curr\u001b[39m=\u001b[39my_curr\u001b[39m.\u001b[39;49mreshape(\u001b[39m500000\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m3\u001b[39;49m,\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     44\u001b[0m \u001b[39m# print(\"ycurr shape after reshape\", y_curr.shape)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m y_pred_curr\u001b[39m=\u001b[39my_pred_total[:,:,i]\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3000000 into shape (1500000,1)"
     ]
    }
   ],
   "source": [
    "modelsavedpath='/home/ddemler/HNLclassifier/fnn_FeatureRegression/fnn_aug17all_oldloss.pt'\n",
    "modelsave=CustomKinematicNet(input_size=input_dim, hidden_layers=hidden_layers, lenoutput=output_dim)\n",
    "modelsave.load_state_dict(torch.load(modelsavedpath))\n",
    "modelsave.to(device)\n",
    "out_feats=['deltaphi', 'deltaeta', 'deltaR', 'mt', 'norm_mt', 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n",
    "\n",
    "\n",
    "test_dataset = KinematicDataset(num_events=500000, seed=10000)\n",
    "input_dim, output_dim = test_dataset.usefulvariables()\n",
    "test_sampler = EpochSampler(train_dataset)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=320, sampler=test_sampler)\n",
    "\n",
    "residuals=np.array([])\n",
    "y_total=np.array([])\n",
    "y_pred_total=np.array([])\n",
    "for i, (x,y) in enumerate(test_loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    y_pred_total=np.append(y_pred_total, y_pred.cpu().detach().numpy())\n",
    "    y_total=np.append(y_total, y.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    # residuals=np.append(residuals, y_pred.cpu().detach().numpy() - y.cpu().detach().numpy())\n",
    "\n",
    "    # residuals.append(y_pred.cpu().detach().numpy() - y.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "numfeatures=13\n",
    "y_pred_total = y_pred_total.reshape(-1,3,numfeatures)\n",
    "y_total = y_total.reshape(-1,3,numfeatures)\n",
    "\n",
    "# print(y_pred_total.shape)\n",
    "# print(y_total.shape)\n",
    "\n",
    "residuals = [[] for _ in range(numfeatures)]\n",
    "label_values = [[] for _ in range(numfeatures)]\n",
    "\n",
    "for i in range(numfeatures):\n",
    "    y_curr=y_total[:,:,i]\n",
    "    # print(\"ycurr shape before reshape\", y_curr.shape)\n",
    "    y_curr=y_curr.reshape(500000*3,1)\n",
    "    # print(\"ycurr shape after reshape\", y_curr.shape)\n",
    "    y_pred_curr=y_pred_total[:,:,i]\n",
    "    y_pred_curr=y_pred_curr.reshape(500000*3,1)\n",
    "    residuals_curr = y_pred_curr - y_curr\n",
    "    residuals[i]=residuals_curr\n",
    "    label_values[i]=y_curr\n",
    "\n",
    "residuals = [np.array(res_list) for res_list in residuals]  # Convert lists of arrays to arrays\n",
    "# residual_medians = [np.median(res) for res in residuals]\n",
    "residual_std_devs = [np.std(res) for res in residuals]\n",
    "residual_means = [np.mean(res) for res in residuals]\n",
    "\n",
    "\n",
    "num_rows = 4\n",
    "num_cols = 5\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 15))\n",
    "flat_axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(flat_axes[:numfeatures]):\n",
    "    ax.hist(residuals[i], bins=100, edgecolor='k', alpha=0.65)\n",
    "    ax.axvline(x=residual_means[i] + residual_std_devs[i], color='r', linestyle='--', label=f'+1 std = {residual_means[i] + residual_std_devs[i]:.2f})')\n",
    "    ax.axvline(x=residual_means[i] - residual_std_devs[i], color='b', linestyle='--', label=f'-1 std = {residual_means[i] - residual_std_devs[i]:.2f})')\n",
    "    ax.set_title(f'Residuals for {out_feats[i]}')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Residual Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Display the mean value on the plot\n",
    "    mean_text = f\"Mean: {residual_means[i]:.2f}, std: {residual_std_devs[i]:.5f}\"\n",
    "    ax.text(0.6, 0.85, mean_text, transform=ax.transAxes)\n",
    "    \n",
    "\n",
    "for ax in flat_axes[numfeatures:]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
