{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['event', 'genWeight', 'MET_phi', '1_phi', '1_genPartFlav', '2_phi', '2_genPartFlav', '3_phi', '3_genPartFlav', 'charge_1', 'charge_2', 'charge_3', 'pt_1', 'pt_2', 'pt_3', 'pt_MET', 'eta_1', 'eta_2', 'eta_3', 'mass_1', 'mass_2', 'mass_3', 'deltaphi_12', 'deltaphi_13', 'deltaphi_23', 'deltaphi_1MET', 'deltaphi_2MET', 'deltaphi_3MET', 'deltaphi_1(23)', 'deltaphi_2(13)', 'deltaphi_3(12)', 'deltaphi_MET(12)', 'deltaphi_MET(13)', 'deltaphi_MET(23)', 'deltaphi_1(2MET)', 'deltaphi_1(3MET)', 'deltaphi_2(1MET)', 'deltaphi_2(3MET)', 'deltaphi_3(1MET)', 'deltaphi_3(2MET)', 'deltaeta_12', 'deltaeta_13', 'deltaeta_23', 'deltaeta_1(23)', 'deltaeta_2(13)', 'deltaeta_3(12)', 'deltaR_12', 'deltaR_13', 'deltaR_23', 'deltaR_1(23)', 'deltaR_2(13)', 'deltaR_3(12)', 'pt_123', 'mt_12', 'mt_13', 'mt_23', 'mt_1MET', 'mt_2MET', 'mt_3MET', 'mt_1(23)', 'mt_2(13)', 'mt_3(12)', 'mt_MET(12)', 'mt_MET(13)', 'mt_MET(23)', 'mt_1(2MET)', 'mt_1(3MET)', 'mt_2(1MET)', 'mt_2(3MET)', 'mt_3(1MET)', 'mt_3(2MET)', 'mass_12', 'mass_13', 'mass_23', 'mass_123', 'Mt_tot', 'HNL_CM_angle_with_MET_1', 'HNL_CM_angle_with_MET_2', 'W_CM_angle_to_plane_1', 'W_CM_angle_to_plane_2', 'W_CM_angle_to_plane_with_MET_1', 'W_CM_angle_to_plane_with_MET_2', 'HNL_CM_mass_1', 'HNL_CM_mass_2', 'HNL_CM_mass_with_MET_1', 'HNL_CM_mass_with_MET_2', 'W_CM_angle_12', 'W_CM_angle_13', 'W_CM_angle_23', 'W_CM_angle_1MET', 'W_CM_angle_2MET', 'W_CM_angle_3MET', 'n_tauh', 'norm_mt_1(23)', 'norm_mt_2(13)', 'norm_mt_3(12)', 'norm_mt_MET(12)', 'norm_mt_MET(13)', 'norm_mt_MET(23)', 'norm_mt_1(2MET)', 'norm_mt_1(3MET)', 'norm_mt_2(1MET)', 'norm_mt_2(3MET)', 'norm_mt_3(1MET)', 'norm_mt_3(2MET)', 'norm_mt_12', 'norm_mt_13', 'norm_mt_23'])\n",
      "568554\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "base_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "full_folder_path = os.path.join(base_path,\"saved_files\", \"fake_data\")\n",
    "# data_df=pd.read_pickle(os.path.join(full_folder_path,\"Aug7_1mil.pkl\"))\n",
    "with open(os.path.join(full_folder_path,\"Aug7_1mil.pkl\"), 'rb') as f:\n",
    "    clean_data_dict = pickle.load(f)\n",
    "print(clean_data_dict.keys())\n",
    "print(len(clean_data_dict['2_phi']))\n",
    "\n",
    "data_dict=deepcopy(clean_data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1_2', '1_3', '2_3'])\n",
      "568554\n",
      "dict_keys(['MET', '1', '2', '3'])\n",
      "dict_keys(['1_phi', 'charge_1', 'pt_1', 'eta_1', 'mass_1'])\n",
      "568554\n"
     ]
    }
   ],
   "source": [
    "input_data_names_ordered = [\n",
    "    ['MET_phi', 'pt_MET'], \n",
    "    ['1_phi', 'charge_1', 'pt_1', 'eta_1', 'mass_1'], \n",
    "    ['2_phi', 'charge_2', 'pt_2', 'eta_2', 'mass_2'], \n",
    "    ['3_phi', 'charge_3', 'pt_3', 'eta_3', 'mass_3']\n",
    "]\n",
    "input_data_particle_order = ['MET', '1', '2', '3']\n",
    "\n",
    "pair_order = [\"MET_1\", \"MET_2\", \"MET_3\", \"1_2\", \"1_3\", \"2_3\"]\n",
    "used_labels2 = [\n",
    "    ['deltaphi_1MET', 'mt_1MET'], \n",
    "    ['deltaphi_2MET', 'mt_2MET'], \n",
    "    ['deltaphi_3MET', 'mt_3MET'], \n",
    "    ['deltaphi_12', 'deltaeta_12', 'deltaR_12', 'mt_12', 'norm_mt_12'], \n",
    "    ['deltaphi_13', 'deltaeta_13', 'deltaR_13', 'mt_13', 'norm_mt_13'], \n",
    "    ['deltaphi_23', 'deltaeta_23', 'deltaR_23', 'mt_23', 'norm_mt_23']\n",
    "]\n",
    "labels = {}\n",
    "for edge, features in zip(pair_order, used_labels2):\n",
    "    labels[edge] = {feature: data_dict[feature] for feature in features if feature in data_dict}\n",
    "\n",
    "input_data = {}\n",
    "for particle, features in zip(input_data_particle_order, input_data_names_ordered):\n",
    "    input_data[particle] = {feature: data_dict[feature] for feature in features if feature in data_dict}\n",
    "lepton_labels=['1_2', '1_3', '2_3']\n",
    "lepton_dict= {key: labels[key] for key in lepton_labels}\n",
    "\n",
    "print(lepton_dict.keys())\n",
    "print(len(lepton_dict['1_2']['deltaphi_12']))\n",
    "print(input_data.keys())\n",
    "print(input_data['1'].keys())\n",
    "print(len(input_data['1']['pt_1']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_order_for_pair(pair):\n",
    "    # Pair orders\n",
    "    orders = {\n",
    "        '1_2': ['1', '2', '3', 'MET'],\n",
    "        '1_3': ['1', '3', '2', 'MET'],\n",
    "        '2_3': ['2', '3', '1', 'MET'],\n",
    "    }\n",
    "    return orders.get(pair, None)\n",
    "\n",
    "def reorder_and_flatten(input_data_tensor, pair):\n",
    "    # Get the correct order\n",
    "    order = get_order_for_pair(pair)\n",
    "    if not order:\n",
    "        raise ValueError(\"Invalid pair\")\n",
    "\n",
    "    # Reorder the tensor based on the provided order\n",
    "    order_indices = [input_data_particle_order.index(p) for p in order]\n",
    "    reordered_tensor = input_data_tensor[:, order_indices, :]\n",
    "\n",
    "    # Flatten the tensor\n",
    "    flattened_tensor = reordered_tensor.view(input_data_tensor.size(0), -1)\n",
    "\n",
    "    return flattened_tensor\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleDNN(nn.Module):\n",
    "    def __init__(self, feature_per_particle, hidden_layer_sizes, num_kinematic_outputs):\n",
    "        super(ParticleDNN, self).__init__()\n",
    "\n",
    "        # Define the first layer differently, once for each pair\n",
    "        self.first_layers = nn.ModuleDict({\n",
    "            '1_2': nn.Linear(3 * feature_per_particle, hidden_layer_sizes[0]),\n",
    "            '1_3': nn.Linear(3 * feature_per_particle, hidden_layer_sizes[0]),\n",
    "            '2_3': nn.Linear(3 * feature_per_particle, hidden_layer_sizes[0])\n",
    "        })\n",
    "        \n",
    "        # Define the other layers in the usual way\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i + 1]))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_layer_sizes[-1], num_kinematic_outputs)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Collect the outputs from the first layer for each pair\n",
    "        combined_output = []\n",
    "        pairs = ['1_2', '1_3', '2_3']\n",
    "        for pair in pairs:\n",
    "            reordered_x = reorder_and_flatten(x, pair)\n",
    "            out = self.first_layers[pair](reordered_x)\n",
    "            combined_output.append(out)\n",
    "        \n",
    "        # Stack the outputs to continue through the DNN\n",
    "        out = torch.cat(combined_output, dim=1)\n",
    "\n",
    "        # Pass through the remaining layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out = self.relu(layer(out))\n",
    "\n",
    "        # Pass through the output layer\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_features_per_particle = 5 \n",
    "hidden_layer_sizes = [64, 32]  # adjusted to account for the combination\n",
    "num_kinematic_outputs = 5  # e.g., for 'deltaphi_12', 'deltaeta_12', etc.\n",
    "\n",
    "# Create the model\n",
    "model = ParticleDNN(num_features_per_particle, hidden_layer_sizes, num_kinematic_outputs)\n",
    "\n",
    "# Sample data: 100 events, 4 particles (including MET), 5 features each\n",
    "sample_data = torch.randn(100, 4, 5)\n",
    "\n",
    "# Get predictions\n",
    "predictions = model(sample_data)\n",
    "\n",
    "# Output predictions shape\n",
    "print(predictions.shape)  # Should print [100, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_flexible2(nn.Module):\n",
    "    def __init__(self, input_vars, pair_inputdim, hidden_layer_sizes):\n",
    "        super(DNN_flexible2, self).__init__()\n",
    "\n",
    "        layer_sizes = [len(input_vars)-2] + hidden_layer_sizes + [1]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.relu(self.layers[i](x))\n",
    "        return x\n",
    "    \n",
    "class DNN_flexible3(nn.Module):\n",
    "    def __init__(self, input_vars, hidden_layer_sizes, pair_input_dim):\n",
    "        super(DNN_flexible3, self).__init__()\n",
    "\n",
    "        # Initial layer dimension\n",
    "        layer_sizes = [len(input_vars)-2] + hidden_layer_sizes + [1]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # If it's not the first or last layer, adjust the dimension for pair_input\n",
    "            if i == 0:  # First layer\n",
    "                input_dim = layer_sizes[i]\n",
    "            else:  # Middle layers\n",
    "                input_dim = layer_sizes[i] + pair_input_dim\n",
    "                \n",
    "            self.layers.append(nn.Linear(input_dim, layer_sizes[i + 1]))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, pair_input):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            \n",
    "            # If it's not the last layer, and not the first layer, concatenate pair_input\n",
    "            if i != 0 and i < len(self.layers) - 1:\n",
    "                x = torch.cat((x, pair_input), dim=1)\n",
    "                \n",
    "            x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
