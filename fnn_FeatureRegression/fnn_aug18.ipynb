{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Sampler\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../FakeDatasetMaking/')\n",
    "from pair_nomet_creation2 import KinematicDataset, EpochSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsavepath='/home/ddemler/HNLclassifier/saved_files/saved_models/FNN_FeatureRegression/fnn_aug18all.pt'\n",
    "pdsavepath='/home/ddemler/HNLclassifier/saved_files/fnn_featregr/fnn_aug18all.csv'\n",
    "\n",
    "out_feats=['deltaphi', 'deltaeta', 'deltaR', 'mt', 'norm_mt', 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n",
    "tryrel=[ 'mt', 'pt', 'px', 'py', 'pz', 'energy']\n",
    "customlossindices=[idx for idx, feat in enumerate(out_feats) if feat in tryrel]\n",
    "\n",
    "hidden_layers = [16,26,32,32,32,48,52,48,32,32,32,26]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 13\n",
      "8 13\n"
     ]
    }
   ],
   "source": [
    "train_dataset = KinematicDataset(num_events=1000000, seed=0)\n",
    "input_dim, output_dim = train_dataset.usefulvariables()\n",
    "# print(input_dim, output_dim)\n",
    "\n",
    "# print(input_dim, output_dim)\n",
    "\n",
    "train_sampler = EpochSampler(train_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=320, sampler=train_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = KinematicDataset(num_events=500000, seed=10000)\n",
    "input_dim, output_dim = val_dataset.usefulvariables()\n",
    "\n",
    "val_sampler = EpochSampler(val_dataset)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=320, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class KinematicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KinematicNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, int(input_dim*1.5))\n",
    "        self.fc2 = nn.Linear(int(input_dim*1.5), int(input_dim//3))\n",
    "        self.fc3 = nn.Linear(int(input_dim//3), output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class CustomKinematicNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, lenoutput, activation_fn=F.relu):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - input_size (int): Size of the input layer.\n",
    "        - hidden_layers (list of int): Sizes of each hidden layer.\n",
    "        - lenoutput (int): Size of the output layer.\n",
    "        - activation_fn (callable): Activation function to use.\n",
    "        \"\"\"\n",
    "        super(CustomKinematicNet, self).__init__()\n",
    "        \n",
    "        # Create the list of layers\n",
    "        layers = [nn.Linear(input_size, hidden_layers[0])]\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i + 1]))\n",
    "        layers.append(nn.Linear(hidden_layers[-1], lenoutput))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.activation_fn = activation_fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation_fn(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def custom_loss(y_pred, y_true):\n",
    "    se_loss = (y_pred - y_true) ** 2\n",
    "    # print(se_loss.shape)\n",
    "    num_features = int(output_dim)\n",
    "\n",
    "    loss_list=[]\n",
    "\n",
    "    for i in range(num_features):\n",
    "\n",
    "        if i in customlossindices:\n",
    "            RMSE=((y_pred[:,i]-y_true[:,i])/y_true[:,i])**2\n",
    "            mask = (y_true[:,i] > 1)\n",
    "\n",
    "            RMSE_meanloss=torch.mean(RMSE[mask])\n",
    "            MSE_meanloss = torch.mean(se_loss[:,i][~mask])\n",
    "\n",
    "            se_loss[mask, i] = RMSE[mask]\n",
    "\n",
    "            loss_list.append(MSE_meanloss.item())\n",
    "            loss_list.append(RMSE_meanloss.item())\n",
    "        else:\n",
    "            loss = torch.mean(se_loss[:,i])\n",
    "            loss_list.append(loss.item())\n",
    "    \n",
    "    full_loss = torch.mean(se_loss)\n",
    "    return loss_list, full_loss\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def l2_regularization(model, lambda_reg):\n",
    "    l2_reg = 0.0\n",
    "    for W in model.parameters():\n",
    "        l2_reg += torch.sum(W ** 2)\n",
    "    return l2_reg * lambda_reg      \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomKinematicNet(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=72, bias=True)\n",
       "    (2): Linear(in_features=72, out_features=82, bias=True)\n",
       "    (3): Linear(in_features=82, out_features=92, bias=True)\n",
       "    (4): Linear(in_features=92, out_features=102, bias=True)\n",
       "    (5): Linear(in_features=102, out_features=112, bias=True)\n",
       "    (6): Linear(in_features=112, out_features=122, bias=True)\n",
       "    (7): Linear(in_features=122, out_features=132, bias=True)\n",
       "    (8): Linear(in_features=132, out_features=142, bias=True)\n",
       "    (9): Linear(in_features=142, out_features=132, bias=True)\n",
       "    (10): Linear(in_features=132, out_features=102, bias=True)\n",
       "    (11): Linear(in_features=102, out_features=92, bias=True)\n",
       "    (12): Linear(in_features=92, out_features=82, bias=True)\n",
       "    (13): Linear(in_features=82, out_features=13, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden_layers=[64,72,82,92,102,112,122,132,142,132,122,112,102,92,82,72,64]\n",
    "# hidden_layers=[64,72,82,92,102,112,122,132,142,132,102,92,82]\n",
    "\n",
    "model = CustomKinematicNet(input_size=input_dim, hidden_layers=hidden_layers, lenoutput=output_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss', 'val_loss', 'l2sum', 'deltaphi_MSE', 'deltaeta_MSE', 'deltaR_MSE', 'mt_MSE', 'mt_RMSE', 'norm_mt_MSE', 'mass_MSE', 'pt_MSE', 'pt_RMSE', 'eta_MSE', 'phi_MSE', 'px_MSE', 'px_RMSE', 'py_MSE', 'py_RMSE', 'pz_MSE', 'pz_RMSE', 'energy_MSE', 'energy_RMSE']\n"
     ]
    }
   ],
   "source": [
    "out_feats=['deltaphi', 'deltaeta', 'deltaR', 'mt', 'norm_mt', 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n",
    "\n",
    "df_outfeats=[]\n",
    "for i, feat in enumerate(out_feats):\n",
    "    if i in customlossindices:\n",
    "        df_outfeats.append(feat +\"_MSE\")\n",
    "        df_outfeats.append(feat +\"_RMSE\")\n",
    "    else:\n",
    "        df_outfeats.append(feat +\"_MSE\")\n",
    "\n",
    "losses_cols=['train_loss', 'val_loss', 'l2sum']+df_outfeats\n",
    "\n",
    "print(losses_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation took 54.57 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Timing data generation\n",
    "start_time_generation = time.time()\n",
    "\n",
    "# Call your data generation function/method here\n",
    "train_dataset.generate_data()\n",
    "\n",
    "end_time_generation = time.time()\n",
    "generation_duration = end_time_generation - start_time_generation\n",
    "print(f\"Data generation took {generation_duration:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One epoch of training took 166.57 seconds.\n",
      "epoch: 0, train: 2.2959e+03, val: 1.8356e+03, deltaphi_MSE: 8.0645e-01, deltaeta_MSE: 8.4590e-01, deltaR_MSE: 7.6279e-01, mt_MSE: 4.0161e+00, mt_RMSE: 8.6907e-01, norm_mt_MSE: 4.0097e-04, mass_MSE: 6.5630e+03, pt_MSE: nan, pt_RMSE: 5.4325e-01, eta_MSE: 2.4629e+00, phi_MSE: 3.4382e+00, px_MSE: 2.4007e+03, px_RMSE: 3.9028e+01, py_MSE: 2.1103e+03, py_RMSE: 3.3280e+01, pz_MSE: 2.8755e+04, pz_RMSE: 5.6844e+01, energy_MSE: nan, energy_RMSE: 2.7054e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m \u001b[39m# train_featsloss_array = np.array(train_featsloss_list)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# avg_train_featsloss = np.mean(train_featsloss_array, axis=0)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/Dmitri-conda/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/Dmitri-conda/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# loss_fn=nn.MSELoss()\n",
    "\n",
    "\n",
    "out_feats=['deltaphi', 'deltaeta', 'deltaR', 'mt', 'norm_mt', 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n",
    "\n",
    "df_outfeats=[]\n",
    "for i, feat in enumerate(out_feats):\n",
    "    if i in customlossindices:\n",
    "        df_outfeats.append(feat +\"_MSE\")\n",
    "        df_outfeats.append(feat +\"_RMSE\")\n",
    "    else:\n",
    "        df_outfeats.append(feat +\"_MSE\")\n",
    "\n",
    "losses_cols=['train_loss', 'val_loss', 'l2sum']+df_outfeats\n",
    "losses_df=pd.DataFrame(columns=losses_cols)\n",
    "\n",
    "\n",
    "numepochs=10000\n",
    "best_loss=np.inf\n",
    "for epoch in range(numepochs):\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    l2sum=0\n",
    "    # train_featsloss_list=[]\n",
    "    for i, (x, y) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False, position=0, disable=True):\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        y_pred=model(x)\n",
    "        train_featslosssep, original_loss = custom_loss(y_pred, y)\n",
    "        # train_featsloss_list.append(train_featslosssep.cpu().numpy())\n",
    "        l2_loss = l2_regularization(model, lambda_reg=1e-7)\n",
    "        loss = original_loss + l2_loss\n",
    "        l2sum+=l2_loss.item()\n",
    "        train_loss += original_loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # train_featsloss_array = np.array(train_featsloss_list)\n",
    "    # avg_train_featsloss = np.mean(train_featsloss_array, axis=0)\n",
    "    \n",
    "    model.eval()\n",
    "    # patience=20\n",
    "    with torch.no_grad():\n",
    "        x,y=next(iter(val_loader))\n",
    "        # x=val_loader\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        y_pred=model(x)\n",
    "        feats_loss, valloss = custom_loss(y_pred, y)\n",
    "        # valloss=sum(feats_loss)/len(feats_loss)\n",
    "        \n",
    "\n",
    "        if valloss<best_loss:\n",
    "            best_loss=valloss\n",
    "            patience=30\n",
    "            modelsave=deepcopy(model.state_dict())\n",
    "            torch.save(modelsave, modelsavepath)\n",
    "        else:\n",
    "            patience-=1\n",
    "            if patience==0:\n",
    "                print('early stopping')\n",
    "                break\n",
    "    # indice=[3,6,12]\n",
    "    # for idx in indice:\n",
    "    #     feats_loss[idx]=feats_loss[idx].cpu().float()\n",
    "    \n",
    "    loss_strings = [f\"{df_outfeats[i]}: {feats_loss[i]:.4e}\" for i in range(len(df_outfeats))]\n",
    "    loss_summary = \", \".join(loss_strings)\n",
    "    loss_values = [train_loss/len(train_loader), valloss.item(), l2sum]\n",
    "    loss_values.extend(feats_loss)\n",
    "\n",
    "    losses_df.loc[epoch] = loss_values\n",
    "    losses_df.to_csv(pdsavepath)\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"epoch: {epoch}, train: {train_loss/len(train_loader):.4e}, val: {valloss.item():.4e}, {loss_summary}\")\n",
    "\n",
    "    # valloss=valloss.cpu().float()\n",
    "    # valloss2=valloss.item()\n",
    "    # loss_strings = [f\"{out_feats[i]}: {feats_loss[i]:.4e}\" for i in range(len(out_feats))]\n",
    "    # loss_summary = \", \".join(loss_strings)\n",
    "    # loss_values = [train_loss/len(train_loader), valloss2, l2sum]\n",
    "    # loss_values.extend(feats_loss)\n",
    "    # # for losses in loss_values:\n",
    "    #     # print(type(losses))\n",
    "    # losses_df.loc[epoch] = loss_values\n",
    "    # losses_df.to_csv(pdsavepath)\n",
    "    # # losses_df.loc[epoch]=[train_loss/len(train_loader), valloss2, feats_loss[0], feats_loss[1]]\n",
    "    # print(f\"epoch: {epoch}, train: {train_loss/len(train_loader):.4e}, val: {valloss2:.4e}, {loss_summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for seed: 10000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3000000 into shape (1500000,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m y_curr\u001b[39m=\u001b[39my_total[:,:,i]\n\u001b[1;32m     42\u001b[0m \u001b[39m# print(\"ycurr shape before reshape\", y_curr.shape)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m y_curr\u001b[39m=\u001b[39my_curr\u001b[39m.\u001b[39;49mreshape(\u001b[39m500000\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m3\u001b[39;49m,\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     44\u001b[0m \u001b[39m# print(\"ycurr shape after reshape\", y_curr.shape)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m y_pred_curr\u001b[39m=\u001b[39my_pred_total[:,:,i]\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3000000 into shape (1500000,1)"
     ]
    }
   ],
   "source": [
    "modelsavedpath='/home/ddemler/HNLclassifier/fnn_FeatureRegression/fnn_aug17all_oldloss.pt'\n",
    "modelsave=CustomKinematicNet(input_size=input_dim, hidden_layers=hidden_layers, lenoutput=output_dim)\n",
    "modelsave.load_state_dict(torch.load(modelsavedpath))\n",
    "modelsave.to(device)\n",
    "out_feats=['deltaphi', 'deltaeta', 'deltaR', 'mt', 'norm_mt', 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n",
    "\n",
    "\n",
    "test_dataset = KinematicDataset(num_events=500000, seed=10000)\n",
    "input_dim, output_dim = test_dataset.usefulvariables()\n",
    "test_sampler = EpochSampler(train_dataset)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=320, sampler=test_sampler)\n",
    "\n",
    "residuals=np.array([])\n",
    "y_total=np.array([])\n",
    "y_pred_total=np.array([])\n",
    "for i, (x,y) in enumerate(test_loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n",
    "    y_pred_total=np.append(y_pred_total, y_pred.cpu().detach().numpy())\n",
    "    y_total=np.append(y_total, y.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    # residuals=np.append(residuals, y_pred.cpu().detach().numpy() - y.cpu().detach().numpy())\n",
    "\n",
    "    # residuals.append(y_pred.cpu().detach().numpy() - y.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "numfeatures=13\n",
    "y_pred_total = y_pred_total.reshape(-1,3,numfeatures)\n",
    "y_total = y_total.reshape(-1,3,numfeatures)\n",
    "\n",
    "# print(y_pred_total.shape)\n",
    "# print(y_total.shape)\n",
    "\n",
    "residuals = [[] for _ in range(numfeatures)]\n",
    "label_values = [[] for _ in range(numfeatures)]\n",
    "\n",
    "for i in range(numfeatures):\n",
    "    y_curr=y_total[:,:,i]\n",
    "    # print(\"ycurr shape before reshape\", y_curr.shape)\n",
    "    y_curr=y_curr.reshape(500000*3,1)\n",
    "    # print(\"ycurr shape after reshape\", y_curr.shape)\n",
    "    y_pred_curr=y_pred_total[:,:,i]\n",
    "    y_pred_curr=y_pred_curr.reshape(500000*3,1)\n",
    "    residuals_curr = y_pred_curr - y_curr\n",
    "    residuals[i]=residuals_curr\n",
    "    label_values[i]=y_curr\n",
    "\n",
    "residuals = [np.array(res_list) for res_list in residuals]  # Convert lists of arrays to arrays\n",
    "# residual_medians = [np.median(res) for res in residuals]\n",
    "residual_std_devs = [np.std(res) for res in residuals]\n",
    "residual_means = [np.mean(res) for res in residuals]\n",
    "\n",
    "\n",
    "num_rows = 4\n",
    "num_cols = 5\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 15))\n",
    "flat_axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(flat_axes[:numfeatures]):\n",
    "    ax.hist(residuals[i], bins=100, edgecolor='k', alpha=0.65)\n",
    "    ax.axvline(x=residual_means[i] + residual_std_devs[i], color='r', linestyle='--', label=f'+1 std = {residual_means[i] + residual_std_devs[i]:.2f})')\n",
    "    ax.axvline(x=residual_means[i] - residual_std_devs[i], color='b', linestyle='--', label=f'-1 std = {residual_means[i] - residual_std_devs[i]:.2f})')\n",
    "    ax.set_title(f'Residuals for {out_feats[i]}')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Residual Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Display the mean value on the plot\n",
    "    mean_text = f\"Mean: {residual_means[i]:.2f}, std: {residual_std_devs[i]:.5f}\"\n",
    "    ax.text(0.6, 0.85, mean_text, transform=ax.transAxes)\n",
    "    \n",
    "\n",
    "for ax in flat_axes[numfeatures:]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
