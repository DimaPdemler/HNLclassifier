{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "sys.path.append('../utils/')\n",
    "from DD_data_extractor_git import Data_generator, outlier_normalization, generate_random_data, exponential_cdf, outlier_normalization, remove_outliers,flatten_2D_list,bucketize, call_dict_with_list\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from DDkinematic_final import *\n",
    "import os\n",
    "from functools import reduce\n",
    "from operator import iconcat\n",
    "from numbers import Number\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from numpy.random import choice\n",
    "from numpy import ravel, unique, array, empty, concatenate, ones, logical_and\n",
    "from numpy import abs as np_abs\n",
    "import vector\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vars_pair_nomet = [ \n",
    "                  'pt_1', 'pt_2', 'pt_3', 'pt_MET', \n",
    "                  'eta_1', 'eta_2', 'eta_3',\n",
    "                  'mass_1', 'mass_2', 'mass_3',\n",
    "                   \n",
    "                  'deltaphi_12', 'deltaphi_13', 'deltaphi_23', \n",
    "                  \n",
    "                  'deltaeta_12', 'deltaeta_13', 'deltaeta_23', \n",
    "\n",
    "                  'deltaR_12', 'deltaR_13', 'deltaR_23', \n",
    "                  'mt_12', 'mt_13', 'mt_23', \n",
    "                  \n",
    "                  'mass_12', 'mass_13', 'mass_23']\n",
    "\n",
    "input_data_names_ordered = [\n",
    "    ['MET_phi', 'pt_MET'], \n",
    "    ['1_phi', 'pt_1', 'eta_1', 'mass_1'], \n",
    "    ['2_phi', 'pt_2', 'eta_2', 'mass_2'], \n",
    "    ['3_phi', 'pt_3', 'eta_3', 'mass_3']\n",
    "]\n",
    "input_data_particle_order = ['MET', '1', '2', '3']\n",
    "\n",
    "pair_order = [\"MET_1\", \"MET_2\", \"MET_3\", \"1_2\", \"1_3\", \"2_3\"]\n",
    "# used_labels2 = [\n",
    "#     ['deltaphi_1MET', 'mt_1MET'], \n",
    "#     ['deltaphi_2MET', 'mt_2MET'], \n",
    "#     ['deltaphi_3MET', 'mt_3MET'], \n",
    "#     ['deltaphi_12', 'deltaeta_12'], \n",
    "#     ['deltaphi_13', 'deltaeta_13'], \n",
    "#     [ 'deltaphi_23', 'deltaeta_23']\n",
    "# ]\n",
    "used_labels2 = [\n",
    "    ['deltaphi_1MET', 'mt_1MET'], \n",
    "    ['deltaphi_2MET', 'mt_2MET'], \n",
    "    ['deltaphi_3MET', 'mt_3MET'], \n",
    "    ['deltaphi_12', 'deltaeta_12', 'deltaR_12', 'mt_12', 'norm_mt_12'], \n",
    "    ['deltaphi_13', 'deltaeta_13', 'deltaR_13', 'mt_13', 'norm_mt_13'], \n",
    "    ['deltaphi_23', 'deltaeta_23', 'deltaR_23', 'mt_23', 'norm_mt_23']\n",
    "]\n",
    "\n",
    "features_toadd=[ 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 15 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting results: 100%|██████████| 15/15 [00:00<00:00, 6515.59it/s]\n",
      "Applying functions: 25it [00:00, 679.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (10000,) (10000,) (10000,) (10000,)\n",
      "(10000,) (10000,) (10000,) (10000,) (10000,)\n",
      "(10000,) (10000,) (10000,) (10000,) (10000,)\n",
      "events, particles, input features:  (10000, 3, 4)\n",
      "events, particle pairs, output kin. features:  (10000, 3, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class KinematicDataset(Dataset):\n",
    "    def __init__(self,num_events,seed=0, used_labels=used_labels2, features_toadd=features_toadd, batch_size=320):\n",
    "        self.num_events = num_events\n",
    "        self.seed = seed\n",
    "        self.output_vars = output_vars_pair_nomet\n",
    "        self.functions =[None, None, None, None,     # pts\n",
    "                    None, None, None,           # etas\n",
    "                    None, None, None,           # masses\n",
    "\n",
    "                    deltaphi, deltaphi, deltaphi,\n",
    "                    \n",
    "                    deltaeta, deltaeta, deltaeta,\n",
    "\n",
    "                    deltaR, deltaR, deltaR, \n",
    "\n",
    "                    transverse_mass, transverse_mass, transverse_mass, \n",
    "                    \n",
    "                    invariant_mass, invariant_mass, invariant_mass\n",
    "                    ]\n",
    "        self.raw_vars_general = [ 'MET_pt', 'MET_phi']\n",
    "       \n",
    "        self.lepton_input_ordered = input_data_names_ordered[1:]\n",
    "        self.lepton_output_ordered = used_labels[3:]\n",
    "        self.lepton_pair_order = pair_order[3:]\n",
    "        self.lepton_particle_order = input_data_particle_order[1:]\n",
    "        self.lepton_specific = ['_eta', '_mass', '_phi', '_pt', '_charge', '_genPartFlav']\n",
    "        self.features_toadd=features_toadd\n",
    "        self.batch_size=batch_size\n",
    "        # raw_vars_lepton1 = lepton_specific\n",
    "        # raw_vars_lepton2 = lepton_specific\n",
    "        # raw_vars_lepton3 = lepton_specific\n",
    "        self.input_vars = [['1_pt'], ['2_pt'], ['3_pt'], ['MET_pt'],\n",
    "\t\t\t        ['1_eta'], ['2_eta'], ['3_eta'], \n",
    "\t\t\t        ['1_mass'], ['2_mass'], ['3_mass'], \n",
    "\t\t\t        ['1_phi', '2_phi'], ['1_phi', '3_phi'], ['2_phi', '3_phi'], \n",
    "\t\t\t         \n",
    "\t\t\t        \n",
    "                    ['1_eta', '2_eta'], ['1_eta', '3_eta'], ['2_eta', '3_eta'], \n",
    "\n",
    "\t\t\t        ['1_eta', '2_eta', '1_phi', '2_phi'], ['1_eta', '3_eta', '1_phi', '3_phi'], ['2_eta', '3_eta', '2_phi', '3_phi'], \n",
    "                     \n",
    "\t\t\t        ['1_pt', '2_pt', '1_phi', '2_phi'], ['1_pt', '3_pt', '1_phi', '3_phi'], ['2_pt', '3_pt', '2_phi', '3_phi'],\n",
    "\t\t\t        \n",
    "\t\t\t        [['1_pt', '2_pt'],['1_phi', '2_phi'],['1_eta', '2_eta'], ['1_mass', '2_mass']], [['1_pt', '3_pt'],['1_phi', '3_phi'],['1_eta', '3_eta'], ['1_mass', '3_mass']], [['2_pt', '3_pt'],['2_phi', '3_phi'],['2_eta', '3_eta'], ['2_mass', '3_mass']]\t\n",
    "                    ]\n",
    "\n",
    "        \n",
    "    \n",
    "    def __getitem__(self,index, normalize=True):\n",
    "        seedvalue=self.seed+15*index\n",
    "        # np.random.seed(self.seed+15*index)\n",
    "        self.data = self.generate_fake_data2(self.num_events, seed_start=seedvalue)\n",
    "\n",
    "        old_keys = [f\"{i}_{var}\" for i in range(1, 4) for var in ['pt', 'eta', 'mass']]\n",
    "        new_keys = [f\"{var}_{i}\" for i in range(1, 4) for var in ['pt', 'eta', 'mass']]\n",
    "\n",
    "        for old_key, new_key in zip(old_keys, new_keys):\n",
    "            if old_key in self.data:\n",
    "                self.data[new_key] = self.data[old_key]\n",
    "                del self.data[old_key]  # Remove old key-value pair from the dictionary\n",
    "            \n",
    "        if normalize:\n",
    "            self.add_norm_features()\n",
    "        # return self.data\n",
    "\n",
    "        l_input, l_output=self.convert_to_array()\n",
    "        l_output2= np.concatenate((l_output, self.add_vectorfeats()), axis=2)\n",
    "        del self.data\n",
    "\n",
    "        pair_input_order=[(0,1),(0,2),(1,2), (1,0),(2,0),(2,1)]\n",
    "\n",
    "        output_dim=3*self.l_output2_shape[2]\n",
    "        input_dim=l_input.shape[2]*6*2\n",
    "        # datashape=(numevents*len(pair_input_order),l_input.shape[2]*2)\n",
    "        datashape=(self.num_events,input_dim)\n",
    "        print(\"datashape\",datashape)\n",
    "        data=np.array(np.zeros(datashape))\n",
    "        for i in range(len(self.pair_input_order)):\n",
    "            combined=np.concatenate((l_input[:,self.pair_input_order[i][0],:],l_input[:,self.pair_input_order[i][1],:]),axis=1)\n",
    "            # print(combined.shape)\n",
    "            #add to data\n",
    "            data[:,i*combined.shape[1]:(i+1)*combined.shape[1]]=combined\n",
    "        del l_input\n",
    "        labels=l_output2.reshape((self.num_events,output_dim))\n",
    "        \n",
    "        x_tensor = torch.from_numpy(data).float()\n",
    "        y_tensor = torch.from_numpy(labels).float()\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "                # return l_input, l_output2\n",
    "            \n",
    "    def generate_data(self):\n",
    "        seedvalue=self.seed\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def worker(instance, start, end, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        data_chunk={var: [] for var in (instance.raw_vars_general + [f'{i}_{var}' for i in range(1, 4) for var in ['eta', 'mass', 'phi', 'pt', ]] + instance.flat_output_vars)}\n",
    "\n",
    "\n",
    "        inputs_chunk= {var: [] for sublist in instance.input_vars for var in (sublist if isinstance(sublist[0], str) else sublist[0])}\n",
    "        pt_dict={'pt_1': [0.02536545873792836, 0.4934279110259645], 'pt_2': [0.019151151336495566, 0.3995434049215345], 'pt_3': [0.023038543045718854, 0.31375795899486003], 'pt_MET': [0.014081741982300087, 0.13542242088536358]}\n",
    "\n",
    "        for i in range(start, end):\n",
    "            sample = {}\n",
    "          \n",
    "            eta_low, eta_high = -2.5, 2.5\n",
    "            mass_low, mass_high = 0, 11\n",
    "            phi_low, phi_high = -np.pi, np.pi\n",
    "            # pt_low, pt_high = 0, 1000\n",
    "\n",
    "\n",
    "            for i in range(1, 4):  # For three leptons\n",
    "                eta = np.random.uniform(low=eta_low, high=eta_high)\n",
    "                mass = np.random.uniform(low=mass_low, high=mass_high)\n",
    "                phi = np.random.uniform(low=phi_low, high=phi_high)\n",
    "                # pt = np.random.uniform(low=pt_low, high=pt_high)\n",
    "                pt=generate_random_data(pt_dict[f'pt_{i}'][0], pt_dict[f'pt_{i}'][1])\n",
    "               \n",
    "\n",
    "                sample[f'{i}_eta'] = eta\n",
    "                sample[f'{i}_mass'] = mass\n",
    "                sample[f'{i}_phi'] = phi\n",
    "                sample[f'{i}_pt'] = pt\n",
    "         \n",
    "            for key in sample:\n",
    "                inputs_chunk[key].append(sample[key])\n",
    "\n",
    "            for key, value in sample.items():\n",
    "                data_chunk[key].append(value)\n",
    "\n",
    "        return data_chunk, inputs_chunk\n",
    "\n",
    "    def generate_fake_data2(self, num_samples, seed_start=0):\n",
    "        self.flat_output_vars=[]\n",
    "        for sublist in self.output_vars:\n",
    "            if isinstance(sublist, list):\n",
    "                for item in sublist:\n",
    "                    self.flat_output_vars.append(item)\n",
    "            else:\n",
    "                self.flat_output_vars.append(sublist)\n",
    "        data = {var: [] for var in (self.raw_vars_general + [f'{i}_{var}' for i in range(1, 4) for var in ['eta', 'mass', 'phi', 'pt']] + self.flat_output_vars)}\n",
    "\n",
    "        inputs = {var: [] for sublist in self.input_vars for var in (sublist if isinstance(sublist[0], str) else sublist[0])}\n",
    "        pt_dict={'pt_1': [0.02536545873792836, 0.4934279110259645], 'pt_2': [0.019151151336495566, 0.3995434049215345], 'pt_3': [0.023038543045718854, 0.31375795899486003], 'pt_MET': [0.014081741982300087, 0.13542242088536358]}\n",
    "\n",
    "        num_chunks = os.cpu_count()  # or any other number based on your preference\n",
    "        if num_chunks > 15: num_chunks = 15\n",
    "        print(f'Using {num_chunks} workers')\n",
    "        chunk_size = num_samples // num_chunks\n",
    "\n",
    "        futures = []\n",
    "        # seeds=[1,2,3,5,6,7]\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            for i in range(num_chunks):\n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size if i != num_chunks - 1 else num_samples\n",
    "                futures.append(executor.submit(self.worker, self, start, end, seed=seed_start+i))\n",
    "\n",
    "\n",
    "\n",
    "        # Collect results from all workers\n",
    "        for future in tqdm(futures, desc='Collecting results'):\n",
    "            chunk_data, chunk_inputs = future.result()\n",
    "            for key, value in chunk_data.items():\n",
    "                data[key].extend(value)\n",
    "            for key, value in chunk_inputs.items():\n",
    "                inputs[key].extend(value)\n",
    "        \n",
    "        tq2=tqdm(enumerate(self.functions), desc='Applying functions')\n",
    "        for i, func in tq2:\n",
    "            if func is not None:\n",
    "                func_inputs = [np.array(call_dict_with_list(inputs, var)) for var in self.input_vars[i]]\n",
    "\n",
    "\n",
    "                func_outputs = func(*func_inputs)\n",
    "\n",
    "                # Add outputs to data\n",
    "                if isinstance(self.output_vars[i], list):\n",
    "                    for j, v in enumerate(self.output_vars[i]):\n",
    "                        if len(data[v]) == 0:\n",
    "                            data[v] = func_outputs[j]\n",
    "                        else:\n",
    "                            data[v] = np.concatenate((data[v], func_outputs[j]))\n",
    "                else:\n",
    "                    if len(data[self.output_vars[i]]) == 0:\n",
    "                        data[self.output_vars[i]] = func_outputs\n",
    "                    else:\n",
    "                        data[self.output_vars[i]] = np.concatenate((data[self.output_vars[i]], func_outputs))\n",
    "        # for key in sample:\n",
    "        #     data[key].append(sample[key])\n",
    "        \n",
    "        for key in data:\n",
    "            data[key] = np.array(data[key])\n",
    "        return data\n",
    "    \n",
    "    def add_norm_features(self):\n",
    "        feat_toadd=[ 'norm_mt_12', 'norm_mt_13', 'norm_mt_23']\n",
    "        feat_orig=feat_toadd.copy()\n",
    "        feat_orig = [i.replace('norm_', '') for i in feat_orig]\n",
    "        for i, feat in enumerate(feat_toadd):\n",
    "            # fake_ptMet=np.asarray\n",
    "            shape_of_other_arrays = self.data['pt_1'].shape\n",
    "            self.data['pt_MET'] = np.zeros(shape_of_other_arrays)\n",
    "            print(self.data['pt_1'].shape, self.data['pt_2'].shape, self.data['pt_3'].shape, self.data['pt_MET'].shape, self.data[feat_orig[i]].shape)\n",
    "            self.data[feat] = outlier_normalization(self.data['pt_1'], self.data['pt_2'], self.data['pt_3'], self.data['pt_MET'], self.data[feat_orig[i]])\n",
    "        return\n",
    "\n",
    "    def convert_to_array(self):\n",
    "        l_input_shape=(self.num_events,len(self.lepton_input_ordered), len(self.lepton_input_ordered[0]))\n",
    "        print(\"events, particles, input features: \",l_input_shape)\n",
    "        l_input= np.empty(l_input_shape)\n",
    "\n",
    "        for i in range(len(self.lepton_input_ordered)):\n",
    "            for j, feature in enumerate(self.lepton_input_ordered[i]):\n",
    "                l_input[:,i,j] = self.data[feature]\n",
    "        l_output_shape=(self.num_events, len(self.lepton_output_ordered), len(self.lepton_output_ordered[0]))\n",
    "        print(\"events, particle pairs, output kin. features: \",l_output_shape)\n",
    "        l_output= np.empty(l_output_shape)\n",
    "\n",
    "        for i in range(len(self.lepton_output_ordered)):\n",
    "            for j, feature in enumerate(self.lepton_output_ordered[i]):\n",
    "                l_output[:,i,j] = self.data[feature]\n",
    "\n",
    "        return l_input, l_output\n",
    "    \n",
    "    def add_vectorfeats(self):\n",
    "        p1_pt=self.data['pt_1']\n",
    "        p2_pt=self.data['pt_2']\n",
    "        p3_pt=self.data['pt_3']\n",
    "\n",
    "        p1_phi=self.data[\"1_phi\"]\n",
    "        p2_phi=self.data[\"2_phi\"]\n",
    "        p3_phi=self.data[\"3_phi\"]\n",
    "\n",
    "        p1_eta=self.data[\"eta_1\"]\n",
    "        p2_eta=self.data[\"eta_2\"]\n",
    "        p3_eta=self.data[\"eta_3\"]\n",
    "\n",
    "        p1_mass=self.data[\"mass_1\"]\n",
    "        p2_mass=self.data[\"mass_2\"]\n",
    "        p3_mass=self.data[\"mass_3\"]\n",
    "\n",
    "        particle1=vector.arr({\"pt\": p1_pt, \"phi\": p1_phi, \"eta\": p1_eta, \"mass\": p1_mass})\n",
    "        particle2=vector.arr({\"pt\": p2_pt, \"phi\": p2_phi, \"eta\": p2_eta, \"mass\": p2_mass})\n",
    "        particle3=vector.arr({\"pt\": p3_pt, \"phi\": p3_phi, \"eta\": p3_eta, \"mass\": p3_mass})\n",
    "\n",
    "        p4_mother12=particle1+particle2\n",
    "        p4_mother23=particle2+particle3\n",
    "        p4_mother13=particle1+particle3\n",
    "\n",
    "        pairs=['12','13','23']\n",
    "        motherpairs=[p4_mother12, p4_mother13, p4_mother23]\n",
    "        # features_toadd=[ 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n",
    "        # features_toadd=[ 'mass', 'pt', 'eta']\n",
    "\n",
    "        add_feat_size=(len(self.data['pt_1']), len(pairs), len(self.features_toadd))\n",
    "        add_feat_array= np.empty(add_feat_size)\n",
    "\n",
    "        for feature in self.features_toadd:\n",
    "            for i, pair in enumerate(pairs):\n",
    "                add_feat_array[:, i, self.features_toadd.index(feature)] = getattr(motherpairs[i], feature)\n",
    "        return add_feat_array\n",
    "\n",
    "    def create_tensor(data,labels):\n",
    "        x_tensor = torch.from_numpy(data).float()\n",
    "        y_tensor = torch.from_numpy(labels).float()\n",
    "\n",
    "        return TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kd=KinematicDataset(10000,seed=0)\n",
    "\n",
    "d1,d2=kd[0]\n",
    "# data2=kd[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3, 4)\n",
      "(10000, 3, 13)\n"
     ]
    }
   ],
   "source": [
    "print(d1.shape)\n",
    "print(d2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(data1\u001b[39m.\u001b[39;49mkeys())\n\u001b[1;32m      2\u001b[0m \u001b[39m# print(\"max and min 1_eta data1\", max(data1['eta_1']), min(data1['eta_1']))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# print(\"max and min 1_eta data2\", max(data2['eta_1']), min(data2['eta_1']))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m eta_1diff\u001b[39m=\u001b[39m data1[\u001b[39m'\u001b[39m\u001b[39meta_1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m-\u001b[39mdata2[\u001b[39m'\u001b[39m\u001b[39meta_1\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data1.keys())\n",
    "# print(\"max and min 1_eta data1\", max(data1['eta_1']), min(data1['eta_1']))\n",
    "# print(\"max and min 1_eta data2\", max(data2['eta_1']), min(data2['eta_1']))\n",
    "eta_1diff= data1['eta_1']-data2['eta_1']\n",
    "print(\"max and min 1_eta diff\", max(eta_1diff), min(eta_1diff))\n",
    "# print(\"data1 charge_1\", data1['charge_1'])\n",
    "deltaphi_13_diff= data1['deltaphi_13']-data2['deltaphi_13']\n",
    "print(\"max and min deltaphi_13 diff\", max(deltaphi_13_diff), min(deltaphi_13_diff))\n",
    "print(type(data1['deltaphi_13']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
