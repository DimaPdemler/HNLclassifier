{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from copy import deepcopy\n",
    "import sys\n",
    "import os\n",
    "# from functools import reduce\n",
    "# from operator import iconcat\n",
    "# from numbers import Number\n",
    "# from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "# from numpy.random import choice\n",
    "# from numpy import ravel, unique, array, empty, concatenate, ones, logical_and\n",
    "# from numpy import abs as np_abs\n",
    "import vector\n",
    "import torch\n",
    "import pickle\n",
    "# import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "sys.path.append('../utils/')\n",
    "from DD_data_extractor_git import Data_generator, outlier_normalization, generate_random_data, exponential_cdf, outlier_normalization, remove_outliers,flatten_2D_list,bucketize, call_dict_with_list\n",
    "\n",
    "from DDkinematic_final import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vars_pair_nomet = [ \n",
    "                  'pt_1', 'pt_2', 'pt_3', 'pt_MET', \n",
    "                  'eta_1', 'eta_2', 'eta_3',\n",
    "                  'mass_1', 'mass_2', 'mass_3',\n",
    "                   \n",
    "                  'deltaphi_12', 'deltaphi_13', 'deltaphi_23', \n",
    "                  \n",
    "                  'deltaeta_12', 'deltaeta_13', 'deltaeta_23', \n",
    "\n",
    "                  'deltaR_12', 'deltaR_13', 'deltaR_23', \n",
    "                  'mt_12', 'mt_13', 'mt_23', \n",
    "                  \n",
    "                  'mass_12', 'mass_13', 'mass_23']\n",
    "\n",
    "input_data_names_ordered = [\n",
    "    ['MET_phi', 'pt_MET'], \n",
    "    ['1_phi', 'pt_1', 'eta_1', 'mass_1'], \n",
    "    ['2_phi', 'pt_2', 'eta_2', 'mass_2'], \n",
    "    ['3_phi', 'pt_3', 'eta_3', 'mass_3']\n",
    "]\n",
    "input_data_particle_order = ['MET', '1', '2', '3']\n",
    "\n",
    "pair_order = [\"MET_1\", \"MET_2\", \"MET_3\", \"1_2\", \"1_3\", \"2_3\"]\n",
    "# used_labels2 = [\n",
    "#     ['deltaphi_1MET', 'mt_1MET'], \n",
    "#     ['deltaphi_2MET', 'mt_2MET'], \n",
    "#     ['deltaphi_3MET', 'mt_3MET'], \n",
    "#     ['deltaphi_12', 'deltaeta_12'], \n",
    "#     ['deltaphi_13', 'deltaeta_13'], \n",
    "#     [ 'deltaphi_23', 'deltaeta_23']\n",
    "# ]\n",
    "used_labels2 = [\n",
    "    ['deltaphi_1MET', 'mt_1MET'], \n",
    "    ['deltaphi_2MET', 'mt_2MET'], \n",
    "    ['deltaphi_3MET', 'mt_3MET'], \n",
    "    ['deltaphi_12', 'deltaeta_12', 'deltaR_12', 'mt_12', 'norm_mt_12'], \n",
    "    ['deltaphi_13', 'deltaeta_13', 'deltaR_13', 'mt_13', 'norm_mt_13'], \n",
    "    ['deltaphi_23', 'deltaeta_23', 'deltaR_23', 'mt_23', 'norm_mt_23']\n",
    "]\n",
    "\n",
    "features_toadd=[ 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinematicDataset(Dataset):\n",
    "    def __init__(self,num_events,seed=0, used_labels=used_labels2, features_toadd=features_toadd, batch_size=320):\n",
    "        self.num_events = num_events\n",
    "        self.current_data = None\n",
    "        self.current_labels = None\n",
    "        self.seed = seed\n",
    "        self.output_vars = output_vars_pair_nomet\n",
    "        self.functions =[None, None, None, None,     # pts\n",
    "                    None, None, None,           # etas\n",
    "                    None, None, None,           # masses\n",
    "\n",
    "                    deltaphi, deltaphi, deltaphi,\n",
    "                    \n",
    "                    deltaeta, deltaeta, deltaeta,\n",
    "\n",
    "                    deltaR, deltaR, deltaR, \n",
    "\n",
    "                    transverse_mass, transverse_mass, transverse_mass, \n",
    "                    \n",
    "                    invariant_mass, invariant_mass, invariant_mass\n",
    "                    ]\n",
    "        self.raw_vars_general = [ 'MET_pt', 'MET_phi']\n",
    "       \n",
    "        self.lepton_input_ordered = input_data_names_ordered[1:]\n",
    "        self.lepton_output_ordered = used_labels[3:]\n",
    "        self.lepton_pair_order = pair_order[3:]\n",
    "        self.lepton_particle_order = input_data_particle_order[1:]\n",
    "        self.lepton_specific = ['_eta', '_mass', '_phi', '_pt', '_charge', '_genPartFlav']\n",
    "        self.features_toadd=features_toadd\n",
    "        self.batch_size=batch_size\n",
    "        # raw_vars_lepton1 = lepton_specific\n",
    "        # raw_vars_lepton2 = lepton_specific\n",
    "        # raw_vars_lepton3 = lepton_specific\n",
    "        self.input_vars = [['1_pt'], ['2_pt'], ['3_pt'], ['MET_pt'],\n",
    "\t\t\t        ['1_eta'], ['2_eta'], ['3_eta'], \n",
    "\t\t\t        ['1_mass'], ['2_mass'], ['3_mass'], \n",
    "\t\t\t        ['1_phi', '2_phi'], ['1_phi', '3_phi'], ['2_phi', '3_phi'], \n",
    "\t\t\t         \n",
    "\t\t\t        \n",
    "                    ['1_eta', '2_eta'], ['1_eta', '3_eta'], ['2_eta', '3_eta'], \n",
    "\n",
    "\t\t\t        ['1_eta', '2_eta', '1_phi', '2_phi'], ['1_eta', '3_eta', '1_phi', '3_phi'], ['2_eta', '3_eta', '2_phi', '3_phi'], \n",
    "                     \n",
    "\t\t\t        ['1_pt', '2_pt', '1_phi', '2_phi'], ['1_pt', '3_pt', '1_phi', '3_phi'], ['2_pt', '3_pt', '2_phi', '3_phi'],\n",
    "\t\t\t        \n",
    "\t\t\t        [['1_pt', '2_pt'],['1_phi', '2_phi'],['1_eta', '2_eta'], ['1_mass', '2_mass']], [['1_pt', '3_pt'],['1_phi', '3_phi'],['1_eta', '3_eta'], ['1_mass', '3_mass']], [['2_pt', '3_pt'],['2_phi', '3_phi'],['2_eta', '3_eta'], ['2_mass', '3_mass']]\t\n",
    "                    ]\n",
    "        base_path = os.path.dirname(os.getcwd())\n",
    "        raw_data_pickle_file = os.path.join(base_path, 'saved_files', 'extracted_data', 'TEST10_data_Aug3')\n",
    "        self.lower_percentiles, self.upper_percentiles = compute_percentiles_from_pickle(raw_data_pickle_file)\n",
    "        self.generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_events\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # self.generate_data()\n",
    "        x_tensor = torch.from_numpy(self.current_data[index % len(self.current_data)]).float()\n",
    "        y_tensor = torch.from_numpy(self.current_labels[index % len(self.current_labels)]).float()\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "        \n",
    "    def set_seed(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate_data(self, normalize=True):\n",
    "        print(\"generating data\")\n",
    "        data_dictlong = self.generate_fake_data2(int(self.num_events*2))\n",
    "\n",
    "        old_keys = [f\"{i}_{var}\" for i in range(1, 4) for var in ['pt', 'eta', 'mass']]\n",
    "        new_keys = [f\"{var}_{i}\" for i in range(1, 4) for var in ['pt', 'eta', 'mass']]\n",
    "\n",
    "        for old_key, new_key in zip(old_keys, new_keys):\n",
    "            if old_key in data_dictlong:\n",
    "                data_dictlong[new_key] = data_dictlong[old_key]\n",
    "                del data_dictlong[old_key]  # Remove old key-value pair from the dictionary\n",
    "            \n",
    "        if normalize:\n",
    "            data_dictlong = self.add_norm_features(data_dictlong)\n",
    "\n",
    "        data_dictlong = remove_outliers(data_dictlong, self.lower_percentiles, self.upper_percentiles)\n",
    "        # print(\"data dict len\", len(data_dictlong['pt_1']))\n",
    "        data_dict = {key: value[:self.num_events] for key, value in data_dictlong.items()}\n",
    "        # print(\"data dict len\", len(data_dict['pt_1']))\n",
    "        # return data_dict\n",
    "        \n",
    "        l_input, l_output=self.convert_to_array(data_dict)\n",
    "        l_output2= np.concatenate((l_output, self.add_vectorfeats(data_dict)), axis=2)\n",
    "\n",
    "\n",
    "        pair_input_order=[(0,1),(0,2),(1,2), (1,0),(2,0),(2,1)]\n",
    "        l_output2_shape=l_output2.shape\n",
    "        output_dim=3*l_output2_shape[2]\n",
    "        self.input_dim=l_input.shape[2]*6*2\n",
    "        \n",
    "        # datashape=(numevents*len(pair_input_order),l_input.shape[2]*2)\n",
    "        datashape=(int(self.num_events),self.input_dim)\n",
    "        # print(\"datashape\",datashape)\n",
    "        data=np.array(np.zeros(datashape))\n",
    "        for i in range(len(pair_input_order)):\n",
    "            combined=np.concatenate((l_input[:,pair_input_order[i][0],:],l_input[:,pair_input_order[i][1],:]),axis=1)\n",
    "            # print(combined.shape)\n",
    "            #add to data\n",
    "            data[:,i*combined.shape[1]:(i+1)*combined.shape[1]]=combined\n",
    "        \n",
    "        # labels=l_output2.reshape((self.num_events,output_dim)\n",
    "        labels=l_output2.reshape((self.num_events,output_dim))\n",
    "        self.current_data=data\n",
    "        self.current_labels=labels\n",
    "        self.seed +=15\n",
    "\n",
    "    def temp_return_data(self):\n",
    "        return self.current_data, self.current_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def worker(instance, start, end, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        data_chunk={var: [] for var in (instance.raw_vars_general + [f'{i}_{var}' for i in range(1, 4) for var in ['eta', 'mass', 'phi', 'pt', ]] + instance.flat_output_vars)}\n",
    "\n",
    "\n",
    "        inputs_chunk= {var: [] for sublist in instance.input_vars for var in (sublist if isinstance(sublist[0], str) else sublist[0])}\n",
    "        pt_dict={'pt_1': [0.02536545873792836, 0.4934279110259645], 'pt_2': [0.019151151336495566, 0.3995434049215345], 'pt_3': [0.023038543045718854, 0.31375795899486003], 'pt_MET': [0.014081741982300087, 0.13542242088536358]}\n",
    "\n",
    "        for i in range(start, end):\n",
    "            sample = {}\n",
    "          \n",
    "            eta_low, eta_high = -2.5, 2.5\n",
    "            mass_low, mass_high = 0, 11\n",
    "            phi_low, phi_high = -np.pi, np.pi\n",
    "            # pt_low, pt_high = 0, 1000\n",
    "\n",
    "\n",
    "            for i in range(1, 4):  # For three leptons\n",
    "                eta = np.random.uniform(low=eta_low, high=eta_high)\n",
    "                mass = np.random.uniform(low=mass_low, high=mass_high)\n",
    "                phi = np.random.uniform(low=phi_low, high=phi_high)\n",
    "                # pt = np.random.uniform(low=pt_low, high=pt_high)\n",
    "                pt=generate_random_data(pt_dict[f'pt_{i}'][0], pt_dict[f'pt_{i}'][1])\n",
    "               \n",
    "\n",
    "                sample[f'{i}_eta'] = eta\n",
    "                sample[f'{i}_mass'] = mass\n",
    "                sample[f'{i}_phi'] = phi\n",
    "                sample[f'{i}_pt'] = pt\n",
    "         \n",
    "            for key in sample:\n",
    "                inputs_chunk[key].append(sample[key])\n",
    "\n",
    "            for key, value in sample.items():\n",
    "                data_chunk[key].append(value)\n",
    "\n",
    "        return data_chunk, inputs_chunk\n",
    "\n",
    "    def generate_fake_data2(self, num_samples):\n",
    "        seed_start = self.seed\n",
    "        self.flat_output_vars=[]\n",
    "        for sublist in self.output_vars:\n",
    "            if isinstance(sublist, list):\n",
    "                for item in sublist:\n",
    "                    self.flat_output_vars.append(item)\n",
    "            else:\n",
    "                self.flat_output_vars.append(sublist)\n",
    "        data = {var: [] for var in (self.raw_vars_general + [f'{i}_{var}' for i in range(1, 4) for var in ['eta', 'mass', 'phi', 'pt']] + self.flat_output_vars)}\n",
    "\n",
    "        inputs = {var: [] for sublist in self.input_vars for var in (sublist if isinstance(sublist[0], str) else sublist[0])}\n",
    "        pt_dict={'pt_1': [0.02536545873792836, 0.4934279110259645], 'pt_2': [0.019151151336495566, 0.3995434049215345], 'pt_3': [0.023038543045718854, 0.31375795899486003], 'pt_MET': [0.014081741982300087, 0.13542242088536358]}\n",
    "\n",
    "        num_chunks = os.cpu_count()  # or any other number based on your preference\n",
    "        if num_chunks > 15: num_chunks = 15\n",
    "        # print(f'Using {num_chunks} workers')\n",
    "        chunk_size = num_samples // num_chunks\n",
    "\n",
    "        futures = []\n",
    "        # seeds=[1,2,3,5,6,7]\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            for i in range(num_chunks):\n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size if i != num_chunks - 1 else num_samples\n",
    "                futures.append(executor.submit(self.worker, self, start, end, seed=seed_start+i))\n",
    "\n",
    "\n",
    "\n",
    "        # Collect results from all workers\n",
    "        for future in tqdm(futures, desc='Collecting results'):\n",
    "            chunk_data, chunk_inputs = future.result()\n",
    "            for key, value in chunk_data.items():\n",
    "                data[key].extend(value)\n",
    "            for key, value in chunk_inputs.items():\n",
    "                inputs[key].extend(value)\n",
    "        \n",
    "        tq2=tqdm(enumerate(self.functions), desc='Applying functions')\n",
    "        for i, func in tq2:\n",
    "            if func is not None:\n",
    "                func_inputs = [np.array(call_dict_with_list(inputs, var)) for var in self.input_vars[i]]\n",
    "\n",
    "\n",
    "                func_outputs = func(*func_inputs)\n",
    "\n",
    "                # Add outputs to data\n",
    "                if isinstance(self.output_vars[i], list):\n",
    "                    for j, v in enumerate(self.output_vars[i]):\n",
    "                        if len(data[v]) == 0:\n",
    "                            data[v] = func_outputs[j]\n",
    "                        else:\n",
    "                            data[v] = np.concatenate((data[v], func_outputs[j]))\n",
    "                else:\n",
    "                    if len(data[self.output_vars[i]]) == 0:\n",
    "                        data[self.output_vars[i]] = func_outputs\n",
    "                    else:\n",
    "                        data[self.output_vars[i]] = np.concatenate((data[self.output_vars[i]], func_outputs))\n",
    "        # for key in sample:\n",
    "        #     data[key].append(sample[key])\n",
    "        \n",
    "        for key in data:\n",
    "            data[key] = np.array(data[key])\n",
    "        return data\n",
    "    \n",
    "    def add_norm_features(self,data_dict):\n",
    "        feat_toadd=[ 'norm_mt_12', 'norm_mt_13', 'norm_mt_23']\n",
    "        feat_orig=feat_toadd.copy()\n",
    "        feat_orig = [i.replace('norm_', '') for i in feat_orig]\n",
    "        for i, feat in enumerate(feat_toadd):\n",
    "            # fake_ptMet=np.asarray\n",
    "            shape_of_other_arrays =data_dict['pt_1'].shape\n",
    "            data_dict['pt_MET'] = np.zeros(shape_of_other_arrays)\n",
    "            # print(data_dict['pt_1'].shape, data_dict['pt_2'].shape, data_dict['pt_3'].shape, data_dict['pt_MET'].shape,data_dict[feat_orig[i]].shape)\n",
    "            data_dict[feat] = outlier_normalization(data_dict['pt_1'], data_dict['pt_2'], data_dict['pt_3'], data_dict['pt_MET'], data_dict[feat_orig[i]])\n",
    "        return data_dict\n",
    "    \n",
    "    def convert_to_array(self, data_dict):\n",
    "        \n",
    "        l_input_shape=(self.num_events,len(self.lepton_input_ordered), len(self.lepton_input_ordered[0]))\n",
    "        # print(\"events, particles, input features: \",l_input_shape)\n",
    "        l_input= np.empty(l_input_shape)\n",
    "\n",
    "        for i in range(len(self.lepton_input_ordered)):\n",
    "            for j, feature in enumerate(self.lepton_input_ordered[i]):\n",
    "                l_input[:,i,j] = data_dict[feature]\n",
    "        l_output_shape=(self.num_events, len(self.lepton_output_ordered), len(self.lepton_output_ordered[0]))\n",
    "        # print(\"events, particle pairs, output kin. features: \",l_output_shape)\n",
    "        l_output= np.empty(l_output_shape)\n",
    "\n",
    "        for i in range(len(self.lepton_output_ordered)):\n",
    "            for j, feature in enumerate(self.lepton_output_ordered[i]):\n",
    "                l_output[:,i,j] = data_dict[feature]\n",
    "\n",
    "        return l_input, l_output\n",
    "    \n",
    "    def add_vectorfeats(self, data_dict):\n",
    "        p1_pt=data_dict['pt_1']\n",
    "        p2_pt=data_dict['pt_2']\n",
    "        p3_pt=data_dict['pt_3']\n",
    "\n",
    "        p1_phi=data_dict[\"1_phi\"]\n",
    "        p2_phi=data_dict[\"2_phi\"]\n",
    "        p3_phi=data_dict[\"3_phi\"]\n",
    "\n",
    "        p1_eta=data_dict[\"eta_1\"]\n",
    "        p2_eta=data_dict[\"eta_2\"]\n",
    "        p3_eta=data_dict[\"eta_3\"]\n",
    "\n",
    "        p1_mass=data_dict[\"mass_1\"]\n",
    "        p2_mass=data_dict[\"mass_2\"]\n",
    "        p3_mass=data_dict[\"mass_3\"]\n",
    "\n",
    "        particle1=vector.arr({\"pt\": p1_pt, \"phi\": p1_phi, \"eta\": p1_eta, \"mass\": p1_mass})\n",
    "        particle2=vector.arr({\"pt\": p2_pt, \"phi\": p2_phi, \"eta\": p2_eta, \"mass\": p2_mass})\n",
    "        particle3=vector.arr({\"pt\": p3_pt, \"phi\": p3_phi, \"eta\": p3_eta, \"mass\": p3_mass})\n",
    "\n",
    "        p4_mother12=particle1+particle2\n",
    "        p4_mother23=particle2+particle3\n",
    "        p4_mother13=particle1+particle3\n",
    "\n",
    "        pairs=['12','13','23']\n",
    "        motherpairs=[p4_mother12, p4_mother13, p4_mother23]\n",
    "        # features_toadd=[ 'mass', 'pt', 'eta' , 'phi',  'px', 'py', 'pz', 'energy']\n",
    "        # features_toadd=[ 'mass', 'pt', 'eta']\n",
    "\n",
    "        add_feat_size=(len(data_dict['pt_1']), len(pairs), len(self.features_toadd))\n",
    "        add_feat_array= np.empty(add_feat_size)\n",
    "\n",
    "        for feature in self.features_toadd:\n",
    "            for i, pair in enumerate(pairs):\n",
    "                add_feat_array[:, i, self.features_toadd.index(feature)] = getattr(motherpairs[i], feature)\n",
    "        return add_feat_array\n",
    "\n",
    "dontremove_outliers=['event', 'genWeight', 'MET_phi', '1_phi', '1_genPartFlav', '2_phi', '2_genPartFlav', '3_phi', '3_genPartFlav', 'charge_1', 'charge_2', 'charge_3', 'pt_1', 'pt_2', 'pt_3', 'pt_MET', 'eta_1', 'eta_2', 'eta_3', 'mass_1', 'mass_2', 'mass_3']\n",
    "\n",
    "def compute_percentiles_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        raw_data_dict = pickle.load(f)\n",
    "    \n",
    "    numeric_data_dict = {k: v for k, v in raw_data_dict.items() if (k not in dontremove_outliers) and np.issubdtype(type(v[0]), np.number)}\n",
    "\n",
    "    # Compute the required percentiles for each numeric feature\n",
    "    lower_percentiles = {k: np.percentile(v, 0.03) for k, v in numeric_data_dict.items()}\n",
    "    upper_percentiles = {k: np.percentile(v, 99.7) for k, v in numeric_data_dict.items()}\n",
    "\n",
    "    return lower_percentiles, upper_percentiles\n",
    "\n",
    "def remove_outliers(data, lower_percentiles, upper_percentiles):\n",
    "    del data['MET_phi']\n",
    "    del data['MET_pt']\n",
    "    outlier_mask = np.zeros(len(next(iter(data.values()))), dtype=bool)\n",
    "    for feature_name, values in data.items():\n",
    "        if (feature_name not in dontremove_outliers) and (feature_name in lower_percentiles):\n",
    "            # print(feature_name)\n",
    "            lower_value = lower_percentiles[feature_name]\n",
    "            upper_value = upper_percentiles[feature_name]\n",
    "            feature_outlier_mask = (np.array(values) < lower_value) | (np.array(values) > upper_value)\n",
    "            outlier_mask |= feature_outlier_mask  # update the outlier mask\n",
    "    \n",
    "    # Remove rows with outliers from all features in the data dictionary\n",
    "    cleaned_data = {k: np.array(v)[~outlier_mask].tolist() for k, v in data.items()}\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class EpochSampler(Sampler):\n",
    "    def __init__(self, data_source, seed=0):\n",
    "        self.data_source = data_source\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Use self.seed to ensure a consistent order within this epoch\n",
    "        np.random.seed(self.seed)\n",
    "        indices = list(np.random.permutation(len(self.data_source)))\n",
    "        # Ensure the dataset generates new data for the next epoch by modifying the seed\n",
    "        self.data_source.iterate_seed()\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting results: 100%|██████████| 15/15 [00:00<00:00, 130.85it/s]\n",
      "Applying functions: 25it [00:01, 19.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dict len 257411\n",
      "data dict len 200000\n"
     ]
    }
   ],
   "source": [
    "dataset = KinematicDataset(num_events=200000, seed=0)\n",
    "sampler = EpochSampler(dataset)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=320, sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting results: 100%|██████████| 15/15 [00:00<00:00, 3880.26it/s]\n",
      "Applying functions: 25it [00:00, 381.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dict len 12905\n",
      "data dict len 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kd=KinematicDataset(10000,seed=0)\n",
    "\n",
    "d1,d2=kd.temp_return_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(10000, 48) (10000, 39)\n"
     ]
    }
   ],
   "source": [
    "print(type(d1), type(d2))\n",
    "print(d1.shape, d2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
