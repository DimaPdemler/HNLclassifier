{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, ReLU\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, GCNConv, global_max_pool, global_mean_pool\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=False\n",
    "filename = \"data_dict_removed_outliersAug4.pkl\" #name of fake data generate pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['event', 'genWeight', 'MET_phi', '1_phi', '1_genPartFlav', '2_phi', '2_genPartFlav', '3_phi', '3_genPartFlav', 'charge_1', 'charge_2', 'charge_3', 'pt_1', 'pt_2', 'pt_3', 'pt_MET', 'eta_1', 'eta_2', 'eta_3', 'mass_1', 'mass_2', 'mass_3', 'deltaphi_12', 'deltaphi_13', 'deltaphi_23', 'deltaphi_1MET', 'deltaphi_2MET', 'deltaphi_3MET', 'deltaphi_1(23)', 'deltaphi_2(13)', 'deltaphi_3(12)', 'deltaphi_MET(12)', 'deltaphi_MET(13)', 'deltaphi_MET(23)', 'deltaphi_1(2MET)', 'deltaphi_1(3MET)', 'deltaphi_2(1MET)', 'deltaphi_2(3MET)', 'deltaphi_3(1MET)', 'deltaphi_3(2MET)', 'deltaeta_12', 'deltaeta_13', 'deltaeta_23', 'deltaeta_1(23)', 'deltaeta_2(13)', 'deltaeta_3(12)', 'deltaR_12', 'deltaR_13', 'deltaR_23', 'deltaR_1(23)', 'deltaR_2(13)', 'deltaR_3(12)', 'pt_123', 'mt_12', 'mt_13', 'mt_23', 'mt_1MET', 'mt_2MET', 'mt_3MET', 'mt_1(23)', 'mt_2(13)', 'mt_3(12)', 'mt_MET(12)', 'mt_MET(13)', 'mt_MET(23)', 'mt_1(2MET)', 'mt_1(3MET)', 'mt_2(1MET)', 'mt_2(3MET)', 'mt_3(1MET)', 'mt_3(2MET)', 'mass_12', 'mass_13', 'mass_23', 'mass_123', 'Mt_tot', 'HNL_CM_angle_with_MET_1', 'HNL_CM_angle_with_MET_2', 'W_CM_angle_to_plane_1', 'W_CM_angle_to_plane_2', 'W_CM_angle_to_plane_with_MET_1', 'W_CM_angle_to_plane_with_MET_2', 'HNL_CM_mass_1', 'HNL_CM_mass_2', 'HNL_CM_mass_with_MET_1', 'HNL_CM_mass_with_MET_2', 'W_CM_angle_12', 'W_CM_angle_13', 'W_CM_angle_23', 'W_CM_angle_1MET', 'W_CM_angle_2MET', 'W_CM_angle_3MET', 'n_tauh', 'norm_mt_1(23)', 'norm_mt_2(13)', 'norm_mt_3(12)', 'norm_mt_MET(12)', 'norm_mt_MET(13)', 'norm_mt_MET(23)', 'norm_mt_1(2MET)', 'norm_mt_1(3MET)', 'norm_mt_2(1MET)', 'norm_mt_2(3MET)', 'norm_mt_3(1MET)', 'norm_mt_3(2MET)', 'norm_mt_12', 'norm_mt_13', 'norm_mt_23'])\n",
      "(85779,)\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the base path and the specific folder where your file is saved\n",
    "base_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "\n",
    "# filename = \"data_dict_removed_outliersAug3.pkl\"\n",
    "\n",
    "\n",
    "if local:\n",
    "    local_path = \"/Users/dimademler/Desktop/UCSD/Labs/EPFL2023/local_gnn_trying/\"\n",
    "    full_file_path = os.path.join(local_path, filename)\n",
    "else:\n",
    "    folder = \"saved_files/fake_data\"\n",
    "    full_file_path = os.path.join(base_path, folder, filename)\n",
    "\n",
    "# Load the data dictionary from the pickle file\n",
    "with open(full_file_path, 'rb') as f:\n",
    "    clean_data_dict = pickle.load(f)\n",
    "print(clean_data_dict.keys())\n",
    "print(clean_data_dict['2_phi'].shape)\n",
    "data_dict=deepcopy(clean_data_dict)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['MET_1', 'MET_2', 'MET_3', '1_2', '1_3', '2_3'])\n",
      "dict_keys(['deltaphi_1MET', 'mt_1MET'])\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the input data and labels according to the notebook\n",
    "input_data_names_ordered = [\n",
    "    ['MET_phi', 'pt_MET'], \n",
    "    ['1_phi', 'charge_1', 'pt_1', 'eta_1', 'mass_1'], \n",
    "    ['2_phi', 'charge_2', 'pt_2', 'eta_2', 'mass_2'], \n",
    "    ['3_phi', 'charge_3', 'pt_3', 'eta_3', 'mass_3']\n",
    "]\n",
    "input_data_particle_order = ['MET', '1', '2', '3']\n",
    "used_labels2 = [\n",
    "    ['deltaphi_1MET', 'mt_1MET'], \n",
    "    ['deltaphi_2MET', 'mt_2MET'], \n",
    "    ['deltaphi_3MET', 'mt_3MET'], \n",
    "    ['deltaphi_12', 'deltaeta_12', 'deltaR_12', 'mt_12', 'norm_mt_12'], \n",
    "    ['deltaphi_13', 'deltaeta_13', 'deltaR_13', 'mt_13', 'norm_mt_13'], \n",
    "    ['deltaphi_23', 'deltaeta_23', 'deltaR_23', 'mt_23', 'norm_mt_23']\n",
    "]\n",
    "edge_order = [\"MET_1\", \"MET_2\", \"MET_3\", \"1_2\", \"1_3\", \"2_3\"]\n",
    "\n",
    "# Create dictionaries for input data and labels\n",
    "input_data = {}\n",
    "for particle, features in zip(input_data_particle_order, input_data_names_ordered):\n",
    "    input_data[particle] = {feature: data_dict[feature] for feature in features if feature in data_dict}\n",
    "\n",
    "labels = {}\n",
    "for edge, features in zip(edge_order, used_labels2):\n",
    "    labels[edge] = {feature: data_dict[feature] for feature in features if feature in data_dict}\n",
    "\n",
    "print(labels.keys())\n",
    "print(labels['MET_1'].keys())\n",
    "\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "def normalize_data(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    return (data - mean) / std\n",
    "\n",
    "inputdatanamess_nocharge=deepcopy(input_data_names_ordered)\n",
    "for i, list in enumerate(input_data_names_ordered):\n",
    "    if \"charge_\"+str(i+1) in list:\n",
    "        inputdatanamess_nocharge[i].remove(\"charge_\"+str(i+1))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "\n",
    "for particle, features in zip(input_data_particle_order, input_data_names_ordered):\n",
    "    for feature in features:\n",
    "        if feature in data_dict:\n",
    "            scaler = StandardScaler()\n",
    "            data = data_dict[feature].reshape(-1, 1)\n",
    "            scaler.fit(data)\n",
    "            data_dict[feature] = scaler.transform(data).flatten()\n",
    "            scalers[(particle, feature)] = scaler\n",
    "\n",
    "for edge, features in zip(edge_order, used_labels2):\n",
    "    for feature in features:\n",
    "        if feature in data_dict:\n",
    "            scaler = StandardScaler()\n",
    "            data = data_dict[feature].reshape(-1, 1)\n",
    "            scaler.fit(data)\n",
    "            data_dict[feature] = scaler.transform(data).flatten()\n",
    "            scalers[(edge, feature)] = scaler\n",
    "\n",
    "for particle, features in zip(input_data_particle_order, input_data_names_ordered):\n",
    "    input_data[particle] = {feature: data_dict[feature] for feature in features if feature in data_dict}\n",
    "\n",
    "for edge, features in zip(edge_order, used_labels2):\n",
    "    labels[edge] = {feature: data_dict[feature] for feature in features if feature in data_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_types: OrderedDict([('MET_1', ['deltaphi_1MET', 'mt_1MET']), ('MET_2', ['deltaphi_2MET', 'mt_2MET']), ('MET_3', ['deltaphi_3MET', 'mt_3MET']), ('1_2', ['deltaphi_12', 'deltaeta_12', 'deltaR_12', 'mt_12', 'norm_mt_12']), ('1_3', ['deltaphi_13', 'deltaeta_13', 'deltaR_13', 'mt_13', 'norm_mt_13']), ('2_3', ['deltaphi_23', 'deltaeta_23', 'deltaR_23', 'mt_23', 'norm_mt_23'])])\n",
      "edge_mapping: {'MET_1': 0, 'MET_2': 1, 'MET_3': 2, '1_2': 3, '1_3': 4, '2_3': 5}\n"
     ]
    }
   ],
   "source": [
    "edge_types = OrderedDict(zip(edge_order, used_labels2))\n",
    "edge_mapping = {edge: i for i, edge in enumerate(edge_types.keys())}\n",
    "\n",
    "print(\"edge_types:\",edge_types)\n",
    "print(\"edge_mapping:\",edge_mapping)\n",
    "\n",
    "edge_type_mapping = {0: 'MET_1', 1: 'MET_2', 2: 'MET_3', 3: '1_2', 4: '1_3', 5: '2_3'}\n",
    "\n",
    "class NodeDNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NodeDNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class EdgeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, edge_types):\n",
    "        super(EdgeModel, self).__init__()\n",
    "        self.edge_networks = nn.ModuleDict({\n",
    "            edge: nn.Sequential(\n",
    "                nn.Linear(2*input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, len(edge_attributes)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            for edge, edge_attributes in edge_types.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, src, dest, edge_type):\n",
    "        outputs = []\n",
    "        for i, edge_type_i in enumerate(edge_type):\n",
    "            src_i, dest_i = src[i], dest[i]\n",
    "            edge_input = torch.cat([src_i, dest_i], dim=-1) \n",
    "            output = self.edge_networks[edge_type_mapping[edge_type_i.item()]](edge_input)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, edge_types, node_input_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.node_dnn_MET = NodeDNN(2, hidden_dim, node_input_dim)\n",
    "        self.node_dnn = NodeDNN(5, hidden_dim, node_input_dim)\n",
    "        self.gcn1 = GCNConv(node_input_dim, 15)\n",
    "        self.gcn2 = GCNConv(15, node_input_dim)\n",
    "        self.edge_model = EdgeModel(node_input_dim, hidden_dim, edge_types)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        row, col = edge_index\n",
    "        out = self.edge_model(x[row], x[col], edge_type)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "node_input_dim=10\n",
    "nodednn_hidden_dim=10\n",
    "\n",
    "node_dnn = NodeDNN(5, nodednn_hidden_dim, node_input_dim)\n",
    "node_dnn_MET = NodeDNN(2, nodednn_hidden_dim, node_input_dim)\n",
    "edge_model = EdgeModel(node_input_dim, nodednn_hidden_dim, edge_types)\n",
    "gnn = GNN(nodednn_hidden_dim, edge_types, node_input_dim)  # change input size to 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_saved = False # Set to true if already preprocessed the data and saved it\n",
    "processed_name=\"fd9.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "events: 100%|██████████| 85779/85779 [00:25<00:00, 3375.02it/s]\n"
     ]
    }
   ],
   "source": [
    "gnn_input_data = []\n",
    "max_label_len = max(len(labels) for labels in used_labels2)\n",
    "# processed_name=\"gnn_processed_1M.pt\"\n",
    "folder2 = os.path.join(base_path, 'saved_files', 'gnnfeatregr_processed_data')\n",
    "\n",
    "if not os.path.exists(folder2):\n",
    "    already_saved = False\n",
    "\n",
    "\n",
    "os.makedirs(folder2, exist_ok=True)\n",
    "\n",
    "savepath= os.path.join(folder2, processed_name)\n",
    "\n",
    "\n",
    "if not already_saved:\n",
    "    for i in tqdm(range(len(data_dict['1_phi'])), desc='events'):\n",
    "        x = []\n",
    "        for j, particle in enumerate(input_data_particle_order):\n",
    "            nodedata = [data_dict[feature][i] for feature in input_data_names_ordered[j]]\n",
    "            nodedata = torch.tensor(nodedata).float()\n",
    "\n",
    "            if j == 0:  # This is the MET node\n",
    "                node = node_dnn_MET(nodedata)\n",
    "            else:  # This is one of the l123 nodes\n",
    "                node = node_dnn(nodedata)\n",
    "            x.append(node)\n",
    "            \n",
    "        # Now all node features should have the same size, and you can stack them\n",
    "        x = torch.stack(x, dim=0)\n",
    "        edges = []\n",
    "        edge_labels = []\n",
    "        edge_types = []\n",
    "        for j, edge in enumerate(edge_order):\n",
    "            particle_mapping = {'MET': 0, '1': 1, '2': 2, '3': 3}\n",
    "            edge_index = [particle_mapping[particle] for particle in edge.split('_')]\n",
    "            edges.append(edge_index)\n",
    "            \n",
    "            edge_label = [data_dict[feature][i] for feature in used_labels2[j]]\n",
    "            edge_label += [-1] * (max_label_len - len(edge_label))  # Padding with -1\n",
    "            edge_label = torch.tensor(edge_label)\n",
    "            edge_labels.append(edge_label)\n",
    "            edge_types.append(edge_mapping[edge])\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        y = torch.stack(edge_labels)\n",
    "        edge_type = torch.tensor(edge_types, dtype=torch.long)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, edge_type=edge_type)\n",
    "        gnn_input_data.append(data)\n",
    "\n",
    "\n",
    "    torch.save(gnn_input_data, savepath)\n",
    "\n",
    "else:\n",
    "    gnn_input_data = torch.load(savepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the actual tensor data for the first Data object\n",
    "# data0 = gnn_input_data[0]\n",
    "# print(\"x:\", data0.x)\n",
    "# print(\"edge_index:\", data0.edge_index)\n",
    "# print(\"edge_attr:\", data0.edge_attr)\n",
    "# print(\"y:\", data0.y)\n",
    "# print(\"edge_type:\", data0.edge_type)\n",
    "\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "train_data, val_data = train_test_split(gnn_input_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the lists to PyTorch Geometric DataLoader objects\n",
    "train_loader = DataLoader(train_data, batch_size=640, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=320, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: change loss function to logcosh)\n",
    "def custom_loss(outs, targets):\n",
    "    diffs = []\n",
    "    for i in range(0, len(outs), 6):\n",
    "        for j in range(6):\n",
    "            out = outs[i+j]\n",
    "            target = targets[i+j]\n",
    "            mask = target != -1  # Mask to ignore padding values\n",
    "            filtered_target = target[mask]\n",
    "            filtered_out = out[:len(filtered_target)]  # Adjust out to match the size of filtered_target\n",
    "            diff = (filtered_out - filtered_target)**2  # Square differences\n",
    "            diffs.append(diff)\n",
    "    return torch.cat(diffs).mean()  # Compute the mean of all diffs\n",
    "\n",
    "def train(model, data_loader, optimizer, device):\n",
    "    timescolumns= ['outs', 'custom_loss', 'lossbackward', 'optimizerstep', 'total_loss']\n",
    "    # times=pd.DataFrame()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc=\"Training batch\"):\n",
    "        batch = batch.to(device)  # Move the batch data to the GPU\n",
    "        optimizer.zero_grad()\n",
    "        outs = model(batch)\n",
    "        loss = custom_loss(outs, batch.y)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def validate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation batch\", disable=True):\n",
    "            batch = batch.to(device)  # Move the batch data to the GPU\n",
    "            outs = model(batch)\n",
    "            val_loss = custom_loss(outs, batch.y)\n",
    "            total_val_loss += val_loss.item()\n",
    "    return total_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: 100%|██████████| 108/108 [03:14<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Training Loss: 107.1940, Validation Loss: 52.6178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: 100%|██████████| 108/108 [03:14<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Training Loss: 105.6409, Validation Loss: 52.4279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch:  31%|███▏      | 34/108 [01:00<02:12,  1.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_epochs), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m'\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m----> 8\u001b[0m     train_loss \u001b[39m=\u001b[39m train(gnn, train_loader, optimizer, device)\n\u001b[1;32m      9\u001b[0m     val_loss \u001b[39m=\u001b[39m validate(gnn, val_loader, device)\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{:03d}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Validation Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, train_loss, val_loss))\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(device)  \u001b[39m# Move the batch data to the GPU\u001b[39;00m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m outs \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m     24\u001b[0m loss \u001b[39m=\u001b[39m custom_loss(outs, batch\u001b[39m.\u001b[39my)\n\u001b[1;32m     25\u001b[0m loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/Dmitri-conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     61\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcn2(x, edge_index)\n\u001b[1;32m     62\u001b[0m row, col \u001b[39m=\u001b[39m edge_index\n\u001b[0;32m---> 63\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_model(x[row], x[col], edge_type)\n\u001b[1;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/Dmitri-conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m, in \u001b[0;36mEdgeModel.forward\u001b[0;34m(self, src, dest, edge_type)\u001b[0m\n\u001b[1;32m     37\u001b[0m     src_i, dest_i \u001b[39m=\u001b[39m src[i], dest[i]\n\u001b[1;32m     38\u001b[0m     edge_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([src_i, dest_i], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[0;32m---> 39\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_networks[edge_type_mapping[edge_type_i\u001b[39m.\u001b[39mitem()]](edge_input)\n\u001b[1;32m     40\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/Dmitri-conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gnn = gnn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(gnn.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc='Epoch', disable=True):\n",
    "    train_loss = train(gnn, train_loader, optimizer, device)\n",
    "    val_loss = validate(gnn, val_loader, device)\n",
    "    print('Epoch: {:03d}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, train_loss, val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7267, 1.0355], [1.4639, 0.8091], [-0.4801, -0.3514], [-0.399, 1.3101, 0.847, 0.5015, 0.5015], [-0.5197, 0.9554, 0.4461, 0.0662, 0.0662], [0.811, -1.0738, -0.1456, 0.7216, 0.7216], [0.1439, 0.7647], [-0.8926, -0.5062], [1.7292, 1.272], [0.9866, 2.3432, 2.3762, -0.2847, -0.2847]]\n",
      "[[0.0, 0.0], [0.236369788646698, 0.0], [0.0, 0.0], [0.0, 0.0, 0.0, 0.10282263159751892, 0.1320360004901886], [0.0, 0.0733170136809349, 0.07204928249120712, 0.0, 0.0], [0.040660008788108826, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0793808251619339], [0.0, 0.14869537949562073], [0.0, 0.5502474308013916], [0.0, 0.02371887117624283, 0.04581897705793381, 0.0, 0.0]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def comparing_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation batch\", disable=True):\n",
    "            batch = batch.to(device)  # Move the batch data to the GPU\n",
    "            outs = model(batch)\n",
    "            labels=batch.y.cpu().detach().numpy()\n",
    "\n",
    "            labels_list = labels.tolist() # convert array to list of lists\n",
    "\n",
    "            labels_cleaned = [[round(item, 4) for item in sublist if item != -1] for sublist in labels_list] # remove -1 values\n",
    "\n",
    "\n",
    "            predictions=[]\n",
    "            for tensor in outs:\n",
    "                values = tensor.cpu().detach().numpy()\n",
    "                values_list = values.tolist()\n",
    "                predictions.append(values_list)\n",
    "            \n",
    "            print(labels_cleaned[:10])\n",
    "            print(predictions[:10])\n",
    "            break\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(comparing_model(gnn, val_loader, device))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
