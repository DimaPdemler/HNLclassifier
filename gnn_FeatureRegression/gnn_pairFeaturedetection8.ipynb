{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, ReLU\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, GCNConv, global_max_pool, global_mean_pool\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=False\n",
    "filename = \"data_dict_removed_outliersAug4.pkl\" #name of fake data generate pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['event', 'genWeight', 'MET_phi', '1_phi', '1_genPartFlav', '2_phi', '2_genPartFlav', '3_phi', '3_genPartFlav', 'charge_1', 'charge_2', 'charge_3', 'pt_1', 'pt_2', 'pt_3', 'pt_MET', 'eta_1', 'eta_2', 'eta_3', 'mass_1', 'mass_2', 'mass_3', 'deltaphi_12', 'deltaphi_13', 'deltaphi_23', 'deltaphi_1MET', 'deltaphi_2MET', 'deltaphi_3MET', 'deltaphi_1(23)', 'deltaphi_2(13)', 'deltaphi_3(12)', 'deltaphi_MET(12)', 'deltaphi_MET(13)', 'deltaphi_MET(23)', 'deltaphi_1(2MET)', 'deltaphi_1(3MET)', 'deltaphi_2(1MET)', 'deltaphi_2(3MET)', 'deltaphi_3(1MET)', 'deltaphi_3(2MET)', 'deltaeta_12', 'deltaeta_13', 'deltaeta_23', 'deltaeta_1(23)', 'deltaeta_2(13)', 'deltaeta_3(12)', 'deltaR_12', 'deltaR_13', 'deltaR_23', 'deltaR_1(23)', 'deltaR_2(13)', 'deltaR_3(12)', 'pt_123', 'mt_12', 'mt_13', 'mt_23', 'mt_1MET', 'mt_2MET', 'mt_3MET', 'mt_1(23)', 'mt_2(13)', 'mt_3(12)', 'mt_MET(12)', 'mt_MET(13)', 'mt_MET(23)', 'mt_1(2MET)', 'mt_1(3MET)', 'mt_2(1MET)', 'mt_2(3MET)', 'mt_3(1MET)', 'mt_3(2MET)', 'mass_12', 'mass_13', 'mass_23', 'mass_123', 'Mt_tot', 'HNL_CM_angle_with_MET_1', 'HNL_CM_angle_with_MET_2', 'W_CM_angle_to_plane_1', 'W_CM_angle_to_plane_2', 'W_CM_angle_to_plane_with_MET_1', 'W_CM_angle_to_plane_with_MET_2', 'HNL_CM_mass_1', 'HNL_CM_mass_2', 'HNL_CM_mass_with_MET_1', 'HNL_CM_mass_with_MET_2', 'W_CM_angle_12', 'W_CM_angle_13', 'W_CM_angle_23', 'W_CM_angle_1MET', 'W_CM_angle_2MET', 'W_CM_angle_3MET', 'n_tauh', 'norm_mt_1(23)', 'norm_mt_2(13)', 'norm_mt_3(12)', 'norm_mt_MET(12)', 'norm_mt_MET(13)', 'norm_mt_MET(23)', 'norm_mt_1(2MET)', 'norm_mt_1(3MET)', 'norm_mt_2(1MET)', 'norm_mt_2(3MET)', 'norm_mt_3(1MET)', 'norm_mt_3(2MET)', 'norm_mt_12', 'norm_mt_13', 'norm_mt_23'])\n",
      "(85779,)\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the base path and the specific folder where your file is saved\n",
    "base_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "\n",
    "# filename = \"data_dict_removed_outliersAug3.pkl\"\n",
    "\n",
    "\n",
    "if local:\n",
    "    local_path = \"/Users/dimademler/Desktop/UCSD/Labs/EPFL2023/local_gnn_trying/\"\n",
    "    full_file_path = os.path.join(local_path, filename)\n",
    "else:\n",
    "    folder = \"saved_files/fake_data\"\n",
    "    full_file_path = os.path.join(base_path, folder, filename)\n",
    "\n",
    "# Load the data dictionary from the pickle file\n",
    "with open(full_file_path, 'rb') as f:\n",
    "    clean_data_dict = pickle.load(f)\n",
    "print(clean_data_dict.keys())\n",
    "print(clean_data_dict['2_phi'].shape)\n",
    "data_dict=deepcopy(clean_data_dict)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['MET_1', 'MET_2', 'MET_3', '1_2', '1_3', '2_3'])\n",
      "dict_keys(['deltaphi_1MET', 'mt_1MET'])\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the input data and labels according to the notebook\n",
    "input_data_names_ordered = [\n",
    "    ['MET_phi', 'pt_MET'], \n",
    "    ['1_phi', 'charge_1', 'pt_1', 'eta_1', 'mass_1'], \n",
    "    ['2_phi', 'charge_2', 'pt_2', 'eta_2', 'mass_2'], \n",
    "    ['3_phi', 'charge_3', 'pt_3', 'eta_3', 'mass_3']\n",
    "]\n",
    "input_data_particle_order = ['MET', '1', '2', '3']\n",
    "used_labels2 = [\n",
    "    ['deltaphi_1MET', 'mt_1MET'], \n",
    "    ['deltaphi_2MET', 'mt_2MET'], \n",
    "    ['deltaphi_3MET', 'mt_3MET'], \n",
    "    ['deltaphi_12', 'deltaeta_12', 'deltaR_12', 'mt_12', 'norm_mt_12'], \n",
    "    ['deltaphi_13', 'deltaeta_13', 'deltaR_13', 'mt_13', 'norm_mt_13'], \n",
    "    ['deltaphi_23', 'deltaeta_23', 'deltaR_23', 'mt_23', 'norm_mt_23']\n",
    "]\n",
    "edge_order = [\"MET_1\", \"MET_2\", \"MET_3\", \"1_2\", \"1_3\", \"2_3\"]\n",
    "\n",
    "# Create dictionaries for input data and labels\n",
    "input_data = {}\n",
    "for particle, features in zip(input_data_particle_order, input_data_names_ordered):\n",
    "    input_data[particle] = {feature: data_dict[feature] for feature in features if feature in data_dict}\n",
    "\n",
    "labels = {}\n",
    "for edge, features in zip(edge_order, used_labels2):\n",
    "    labels[edge] = {feature: data_dict[feature] for feature in features if feature in data_dict}\n",
    "\n",
    "print(labels.keys())\n",
    "print(labels['MET_1'].keys())\n",
    "\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "def normalize_data(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    return (data - mean) / std\n",
    "\n",
    "inputdatanamess_nocharge=deepcopy(input_data_names_ordered)\n",
    "for i, list in enumerate(input_data_names_ordered):\n",
    "    if \"charge_\"+str(i+1) in list:\n",
    "        inputdatanamess_nocharge[i].remove(\"charge_\"+str(i+1))  \n",
    "\n",
    "# for particle, features in zip(input_data_particle_order, inputdatanamess_nocharge):\n",
    "#     input_data[particle] = {feature: normalize_data(data_dict[feature]) for feature in features if feature in data_dict}\n",
    "\n",
    "# for edge, features in zip(edge_order, used_labels2):\n",
    "#     labels[edge] = {feature: normalize_data(data_dict[feature]) for feature in features if feature in data_dict}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "\n",
    "for particle, features in zip(input_data_particle_order, input_data_names_ordered):\n",
    "    for feature in features:\n",
    "        if feature in data_dict:\n",
    "            scaler = StandardScaler()\n",
    "            data = data_dict[feature].reshape(-1, 1)\n",
    "            scaler.fit(data)\n",
    "            data_dict[feature] = scaler.transform(data).flatten()\n",
    "            scalers[(particle, feature)] = scaler\n",
    "\n",
    "for edge, features in zip(edge_order, used_labels2):\n",
    "    for feature in features:\n",
    "        if feature in data_dict:\n",
    "            scaler = StandardScaler()\n",
    "            data = data_dict[feature].reshape(-1, 1)\n",
    "            scaler.fit(data)\n",
    "            data_dict[feature] = scaler.transform(data).flatten()\n",
    "            scalers[(edge, feature)] = scaler\n",
    "\n",
    "for particle, features in zip(input_data_particle_order, input_data_names_ordered):\n",
    "    input_data[particle] = {feature: data_dict[feature] for feature in features if feature in data_dict}\n",
    "\n",
    "for edge, features in zip(edge_order, used_labels2):\n",
    "    labels[edge] = {feature: data_dict[feature] for feature in features if feature in data_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_types: OrderedDict([('MET_1', ['deltaphi_1MET', 'mt_1MET']), ('MET_2', ['deltaphi_2MET', 'mt_2MET']), ('MET_3', ['deltaphi_3MET', 'mt_3MET']), ('1_2', ['deltaphi_12', 'deltaeta_12', 'deltaR_12', 'mt_12', 'norm_mt_12']), ('1_3', ['deltaphi_13', 'deltaeta_13', 'deltaR_13', 'mt_13', 'norm_mt_13']), ('2_3', ['deltaphi_23', 'deltaeta_23', 'deltaR_23', 'mt_23', 'norm_mt_23'])])\n",
      "edge_mapping: {'MET_1': 0, 'MET_2': 1, 'MET_3': 2, '1_2': 3, '1_3': 4, '2_3': 5}\n",
      "20 10\n",
      "20 10\n"
     ]
    }
   ],
   "source": [
    "edge_types = OrderedDict(zip(edge_order, used_labels2))\n",
    "edge_mapping = {edge: i for i, edge in enumerate(edge_types.keys())}\n",
    "\n",
    "print(\"edge_types:\",edge_types)\n",
    "print(\"edge_mapping:\",edge_mapping)\n",
    "\n",
    "edge_type_mapping = {0: 'MET_1', 1: 'MET_2', 2: 'MET_3', 3: '1_2', 4: '1_3', 5: '2_3'}\n",
    "\n",
    "class NodeDNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NodeDNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class EdgeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, edge_types):\n",
    "        # for edge, edge_attributes in edge_types.items():\n",
    "        #     print(2*input_dim + len(edge_attributes))\n",
    "        print(4*input_dim, hidden_dim)\n",
    "        super(EdgeModel, self).__init__()\n",
    "        self.edge_networks = nn.ModuleDict({\n",
    "            edge: nn.Sequential(\n",
    "                nn.Linear(4*input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, len(edge_attributes)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            \n",
    "            for edge, edge_attributes in edge_types.items()\n",
    "        })\n",
    "\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, edge_type):\n",
    "        outputs = []\n",
    "        for i, edge_type_i in enumerate(edge_type):\n",
    "            src_i, dest_i = src[i], dest[i]\n",
    "            edge_attr_i = edge_attr[i]\n",
    "\n",
    "            # print(f\"src_i shape: {src_i.shape}\")\n",
    "            # print(f\"dest_i shape: {dest_i.shape}\")\n",
    "            # print(f\"edge_attr_i shape: {edge_attr_i.shape}\")\n",
    "\n",
    "            # edge_input = torch.cat([src_i, dest_i, edge_attr_i], dim=0)  \n",
    "            edge_input = torch.cat([src_i, dest_i, edge_attr_i], dim=-1) \n",
    "            # print(\"Size of edge_input: \", edge_input.shape) \n",
    "            # print(\"edge_networks:\",self.edge_networks.keys())\n",
    "            output = self.edge_networks[edge_type_mapping[edge_type_i.item()]](edge_input)\n",
    "            outputs.append(output)\n",
    "            # print(\" /output shape:\",output.shape)\n",
    "        # print(\"outputs shape:\",outputs)\n",
    "        return outputs\n",
    "        # return torch.cat(outputs, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, edge_types, node_input_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.node_dnn_MET = NodeDNN(2, hidden_dim, node_input_dim)\n",
    "        self.node_dnn = NodeDNN(5, hidden_dim, node_input_dim)\n",
    "        self.gcn1 = GCNConv(node_input_dim, 15)\n",
    "        self.gcn2 = GCNConv(15, node_input_dim)\n",
    "        self.edge_model = EdgeModel(node_input_dim, hidden_dim, edge_types)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, edge_type = data.x, data.edge_index, data.edge_attr, data.edge_type\n",
    "        # print(\"edge_type:\",edge_type)\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        # Transform the edge attributes\n",
    "        row, col = edge_index\n",
    "        out = self.edge_model(x[row], x[col], edge_attr, edge_type)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "node_input_dim=5\n",
    "nodednn_hidden_dim=10\n",
    "\n",
    "node_dnn = NodeDNN(5, nodednn_hidden_dim, node_input_dim)\n",
    "node_dnn_MET = NodeDNN(2, nodednn_hidden_dim, node_input_dim)\n",
    "edge_model = EdgeModel(node_input_dim, nodednn_hidden_dim, edge_types)\n",
    "gnn = GNN(nodednn_hidden_dim, edge_types, node_input_dim)  # change input size to 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_saved = True # Set to true if already preprocessed the data and saved it\n",
    "processed_name=\"gnn_processed_100k.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_input_data = []\n",
    "max_label_len = max(len(labels) for labels in used_labels2)\n",
    "# processed_name=\"gnn_processed_1M.pt\"\n",
    "folder2 = os.path.join(base_path, 'saved_files', 'gnnfeatregr_processed_data')\n",
    "\n",
    "if not os.path.exists(folder2):\n",
    "    already_saved = False\n",
    "\n",
    "\n",
    "os.makedirs(folder2, exist_ok=True)\n",
    "\n",
    "savepath= os.path.join(folder2, processed_name)\n",
    "\n",
    "\n",
    "if not already_saved:\n",
    "    for i in tqdm(range(len(data_dict['1_phi'])), desc='events'):\n",
    "        x = []\n",
    "        for j, particle in enumerate(input_data_particle_order):\n",
    "            nodedata = [data_dict[feature][i] for feature in input_data_names_ordered[j]]\n",
    "            nodedata = torch.tensor(nodedata).float()\n",
    "\n",
    "            # Transform the node here\n",
    "            if j == 0:  # This is the MET node\n",
    "                node = node_dnn_MET(nodedata)\n",
    "            else:  # This is one of the l123 nodes\n",
    "                node = node_dnn(nodedata)\n",
    "            x.append(node)\n",
    "            \n",
    "        # Now all node features should have the same size, and you can stack them\n",
    "        x = torch.stack(x, dim=0)\n",
    "        edges = []\n",
    "        edge_attrs = []\n",
    "        edge_labels = []\n",
    "        edge_types = []\n",
    "        for j, edge in enumerate(edge_order):\n",
    "            particle_mapping = {'MET': 0, '1': 1, '2': 2, '3': 3}\n",
    "            edge_index = [particle_mapping[particle] for particle in edge.split('_')]\n",
    "            edges.append(edge_index)\n",
    "\n",
    "            # Create the edge attributes by concatenating the node features\n",
    "            node1_features = x[edge_index[0]]\n",
    "            node2_features = x[edge_index[1]]\n",
    "            edge_attr = torch.cat((node1_features, node2_features))\n",
    "            edge_attrs.append(edge_attr)\n",
    "            \n",
    "            edge_label = [data_dict[feature][i] for feature in used_labels2[j]]\n",
    "            edge_label += [-1] * (max_label_len - len(edge_label))  # Padding with -1\n",
    "            edge_label = torch.tensor(edge_label)\n",
    "            edge_labels.append(edge_label)\n",
    "            # edge_types.append(edge) \n",
    "            edge_types.append(edge_mapping[edge])\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.stack(edge_attrs)\n",
    "        y = torch.stack(edge_labels)\n",
    "        edge_type = torch.tensor(edge_types, dtype=torch.long)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, edge_type=edge_type)\n",
    "        gnn_input_data.append(data)\n",
    "\n",
    "    torch.save(gnn_input_data, savepath)\n",
    "\n",
    "else:\n",
    "    gnn_input_data = torch.load(savepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the actual tensor data for the first Data object\n",
    "# data0 = gnn_input_data[0]\n",
    "# print(\"x:\", data0.x)\n",
    "# print(\"edge_index:\", data0.edge_index)\n",
    "# print(\"edge_attr:\", data0.edge_attr)\n",
    "# print(\"y:\", data0.y)\n",
    "# print(\"edge_type:\", data0.edge_type)\n",
    "\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "train_data, val_data = train_test_split(gnn_input_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the lists to PyTorch Geometric DataLoader objects\n",
    "train_loader = DataLoader(train_data, batch_size=640, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=320, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: change loss function to log(tanh)\n",
    "def custom_loss(outs, targets):\n",
    "    diffs = []\n",
    "    for i in range(0, len(outs), 6):\n",
    "        for j in range(6):\n",
    "            out = outs[i+j]\n",
    "            target = targets[i+j]\n",
    "            mask = target != -1  # Mask to ignore padding values\n",
    "            filtered_target = target[mask]\n",
    "            filtered_out = out[:len(filtered_target)]  # Adjust out to match the size of filtered_target\n",
    "            diff = (filtered_out - filtered_target)**2  # Square differences\n",
    "            diffs.append(diff)\n",
    "    return torch.cat(diffs).mean()  # Compute the mean of all diffs\n",
    "\n",
    "def train(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc=\"Training batch\"):\n",
    "        batch = batch.to(device)  # Move the batch data to the GPU\n",
    "        optimizer.zero_grad()\n",
    "        outs = model(batch)\n",
    "        loss = custom_loss(outs, batch.y)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def validate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation batch\", disable=True):\n",
    "            batch = batch.to(device)  # Move the batch data to the GPU\n",
    "            outs = model(batch)\n",
    "            val_loss = custom_loss(outs, batch.y)\n",
    "            total_val_loss += val_loss.item()\n",
    "    return total_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: 100%|██████████| 108/108 [03:26<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Training Loss: 106.9818, Validation Loss: 51.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: 100%|██████████| 108/108 [03:29<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Training Loss: 101.6927, Validation Loss: 50.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: 100%|██████████| 108/108 [03:29<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Training Loss: 99.7730, Validation Loss: 49.4698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: 100%|██████████| 108/108 [03:27<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Training Loss: 98.9140, Validation Loss: 49.1445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: 100%|██████████| 108/108 [03:27<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Training Loss: 98.3380, Validation Loss: 48.9048\n"
     ]
    }
   ],
   "source": [
    "gnn = gnn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(gnn.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc='Epoch', disable=True):\n",
    "    train_loss = train(gnn, train_loader, optimizer, device)\n",
    "    val_loss = validate(gnn, val_loader, device)\n",
    "    print('Epoch: {:03d}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, train_loss, val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
